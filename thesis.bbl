\begin{thebibliography}{100}

\bibitem{agrawal2012analysis}
Shipra Agrawal and Navin Goyal.
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock In {\em Conference on Learning Theory}, pages 39--1, 2012.

\bibitem{ammar2014online}
Haitham~Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew Taylor.
\newblock Online multi-task learning for policy gradient methods.
\newblock In {\em International Conference on Machine Learning}, pages
  1206--1214, 2014.

\bibitem{asadi2016alternative}
Kavosh Asadi and Michael~L. Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1612.05628}, 2016.

\bibitem{pmlr-v70-asadi17a}
Kavosh Asadi and Michael~L. Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  243--252, 2017.

\bibitem{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning}, 47(2-3):235--256, 2002.

\bibitem{auer2007logarithmic}
Peter Auer and Ronald Ortner.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  49--56, 2007.

\bibitem{azizzadenesheli2517efficient}
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar.
\newblock Efficient exploration through bayesian deep q-networks.
\newblock {\em Asteroids}, 2517(1516):108.

\bibitem{bao2008infinite}
Bing-Kun Bao, Bao-Qun Yin, and Hong-Sheng Xi.
\newblock Infinite-horizon policy-gradient estimation with variable discount
  factor for markov decision process.
\newblock In {\em Proc. ICICIC}, pages 584--584. IEEE, 2008.

\bibitem{bellemare13arcade}
M.~G. {Bellemare}, Y.~{Naddaf}, J.~{Veness}, and M.~{Bowling}.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47, jun 2013.

\bibitem{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1471--1479, 2016.

\bibitem{bellemare2013arcade}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem{bellemare2015increasing}
Marc~G. Bellemare, Georg Ostrovski, Arthur Guez, Philip~S. Thomas, and
  R{\'{e}}mi Munos.
\newblock Increasing the action gap: New operators for reinforcement learning.
\newblock In {\em Proceedings of the thirtieth AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem{bellman2013dynamic}
Richard Bellman.
\newblock {\em Dynamic programming}.
\newblock Courier Corporation, 2013.

\bibitem{bertsekas2005dynamic}
Dimitri~P. Bertsekas.
\newblock {\em Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 2005.

\bibitem{bonarini2006self}
Andrea Bonarini, Alessandro Lazaric, Marcello Restelli, and Patrick Vitali.
\newblock Self-development framework for reinforcement learning agents.
\newblock In {\em International Conference on Development and Learning}, volume
  178, pages 355--362, 2006.

\bibitem{brafman2002r}
Ronen~I. Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 3(Oct):213--231, 2002.

\bibitem{gym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem{bubeck2012regret}
S{\'e}bastien Bubeck, Nicolo Cesa-Bianchi, et~al.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  5(1):1--122, 2012.

\bibitem{bworld}
{chainer.org}.
\newblock {ChainerRL}.
\newblock \url{https://github.com/chainer/chainerrl}, 2017.

\bibitem{chapelle2011empirical}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of thompson sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2249--2257, 2011.

\bibitem{chentanez2005intrinsically}
Nuttapong Chentanez, Andrew~G. Barto, and Satinder~P. Singh.
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1281--1288, 2005.

\bibitem{crites1996improving}
Robert~H. Crites and Andrew~G. Barto.
\newblock Improving elevator performance using reinforcement learning.
\newblock In {\em Proc. NIPS}, pages 1017--1023, 1996.

\bibitem{dearden1999model}
Richard Dearden, Nir Friedman, and David Andre.
\newblock Model based bayesian exploration.
\newblock In {\em Proceedings of the Fifteenth conference on Uncertainty in
  artificial intelligence}, pages 150--159. Morgan Kaufmann Publishers Inc.,
  1999.

\bibitem{dearden1998bayesian}
Richard Dearden, Nir Friedman, and Stuart Russell.
\newblock Bayesian q-learning.
\newblock In {\em AAAI}, pages 761--768, 1998.

\bibitem{deramo2017maximum}
Carlo D'Eramo, Alessandro Nuara, Matteo Pirotta, and Marcello Restelli.
\newblock Estimating the maximum expected value in continuous reinforcement
  learning problems.
\newblock In {\em {AAAI}}, pages XXX--XXX. {AAAI} Press, 2017.

\bibitem{deramo2016estimating}
Carlo D'Eramo, Marcello Restelli, and Alessandro Nuara.
\newblock Estimating maximum expected value through gaussian approximation.
\newblock In {\em {ICML}}, volume~48 of {\em {JMLR} Workshop and Conference
  Proceedings}, pages 1032--1040. JMLR.org, 2016.

\bibitem{doya2000reinforcement}
Kenji Doya.
\newblock Reinforcement learning in continuous time and space.
\newblock {\em Neural computation}, 12(1):219--245, 2000.

\bibitem{duan2016benchmarking}
Yan Duan, Xi~Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em International Conference on Machine Learning}, pages
  1329--1338, 2016.

\bibitem{engel2005reinforcement}
Yaakov Engel, Shie Mannor, and Ron Meir.
\newblock Reinforcement learning with gaussian processes.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, pages 201--208. ACM, 2005.

\bibitem{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 6(Apr):503--556, 2005.

\bibitem{EvenDar2001}
Eyal Even-Dar and Yishay Mansour.
\newblock {\em Learning Rates for Q-Learning}, pages 589--604.
\newblock Springer Berlin Heidelberg, 2001.

\bibitem{even2002convergence}
Eyal Even-Dar and Yishay Mansour.
\newblock Convergence of optimistic and incremental q-learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1499--1506, 2002.

\bibitem{franccois2015discount}
Vincent Fran{\c{c}}ois-Lavet, Raphael Fonteneau, and Damien Ernst.
\newblock How to discount deep reinforcement learning: Towards new dynamic
  strategies.
\newblock {\em arXiv preprint arXiv:1512.02011}, 2015.

\bibitem{doi:10.1162/089976600300015204}
J\"{u}rgen Franke and Michael~H. Neumann.
\newblock Bootstrapping neural networks.
\newblock {\em Neural Computation}, 12(8), 2000.

\bibitem{JMLR:v16:geramifard15a}
Alborz Geramifard, Christoph Dann, Robert~H. Klein, William Dabney, and
  Jonathan~P. How.
\newblock Rlpy: A value-function-based reinforcement learning framework for
  education and research.
\newblock {\em Journal of Machine Learning Research}, 16:1573--1578, 2015.

\bibitem{geurts2006extremely}
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
\newblock Extremely randomized trees.
\newblock {\em Machine learning}, 63(1):3--42, 2006.

\bibitem{NIPS2011_4251}
Mohammad Ghavamzadeh, Hilbert~J. Kappen, Mohammad~G. Azar, and R\'{e}mi Munos.
\newblock Speedy q-learning.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, {\em Proc. NIPS}, pages 2411--2419. Curran
  Associates, Inc., 2011.

\bibitem{granmo2010solving}
Ole-Christoffer Granmo.
\newblock Solving two-armed bernoulli bandit problems using a bayesian learning
  automaton.
\newblock {\em International Journal of Intelligent Computing and Cybernetics},
  3(2):207--234, 2010.

\bibitem{grossman1972non}
Michael Grossman and Robert Katz.
\newblock {\em Non-Newtonian Calculus: A Self-contained, Elementary Exposition
  of the Authors' Investigations...}
\newblock Non-Newtonian Calculus, 1972.

\bibitem{hasselt2015double}
Hasselt Hado~van, Guez Arthur, and Silver David.
\newblock Deep reinforcement learning with double q-learning.
\newblock {\em CoRR}, abs/1509.06461, 2015.

\bibitem{higgins2017darla}
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess,
  Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander
  Lerchner.
\newblock Darla: Improving zero-shot transfer in reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1480--1490, 2017.

\bibitem{hoeffding1963probability}
Wassily Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock {\em Journal of the American statistical association},
  58(301):13--30, 1963.

\bibitem{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1563--1600, 2010.

\bibitem{kakade2003sample}
Sham~Machandranath Kakade et~al.
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning}, 49(2-3):209--232, 2002.

\bibitem{Kobayashi2009}
Kunikazu Kobayashi, Hiroyuki Mizoue, Takashi Kuremoto, and Masanao Obayashi.
\newblock {\em A Meta-learning Method Based on Temporal Difference Error},
  pages 530--537.
\newblock Springer Berlin Heidelberg, 2009.

\bibitem{kober2013reinforcement}
Jens Kober, J.~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock {\em The International Journal of Robotics Research},
  32(11):1238--1274, 2013.

\bibitem{kolter2009near}
J.~Zico Kolter and Andrew~Y. Ng.
\newblock Near-bayesian exploration in polynomial time.
\newblock In {\em International Conference on Machine Learning}, pages
  513--520. ACM, 2009.

\bibitem{lai1985asymptotically}
Tze~Leung Lai and Herbert Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics}, 6(1):4--22, 1985.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436, 2015.

\bibitem{lee2013bias}
Daewoo Lee, Boris Defourny, and Warren~B. Powell.
\newblock Bias-corrected q-learning to control max-operator bias in q-learning.
\newblock In {\em Adaptive Dynamic Programming And Reinforcement Learning
  (ADPRL), 2013 IEEE Symposium on}, pages 93--99. IEEE, 2013.

\bibitem{lee2012intelligent}
Donghun Lee and Warren~B Powell.
\newblock An intelligent battery controller using bias-corrected q-learning.
\newblock In {\em AAAI}. Citeseer, 2012.

\bibitem{may2011simulation}
Benedict~C. May and David~S. Leslie.
\newblock Simulation studies in optimistic bayesian sampling in
  contextual-bandit problems.
\newblock {\em Statistics Group, Department of Mathematics, University of
  Bristol}, 11:02, 2011.

\bibitem{meuleau1999exploration}
Nicolas Meuleau and Paul Bourgine.
\newblock Exploration of multi-state environments: Local measures and
  back-propagation of uncertainty.
\newblock {\em Machine Learning}, 35(2):117--154, 1999.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem{mohagheghi2007proportional}
Salman Mohagheghi, Yamille del Valle, Ganesh~Kumar Venayagamoorthy, and
  Ronald~G. Harley.
\newblock A proportional-integrator type adaptive critic design-based
  neurocontroller for a static compensator in a multimachine power system.
\newblock {\em IEEE Transactions on Industrial Electronics}, 54(1):86--96,
  2007.

\bibitem{opitz1999popular}
David Opitz and Richard Maclin.
\newblock Popular ensemble methods: An empirical study.
\newblock {\em Journal of artificial intelligence research}, 11:169--198, 1999.

\bibitem{osband2017deep}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  4026--4034. Curran Associates, Inc., 2016.

\bibitem{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3003--3011, 2013.

\bibitem{osband2016generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock In {\em International Conference on Machine Learning}, pages
  2377--2386, 2016.

\bibitem{ostrovski2017count}
Georg Ostrovski, Marc~G. Bellemare, Aaron van~den Oord, and R{\'e}mi Munos.
\newblock Count-based exploration with neural density models.
\newblock {\em arXiv preprint arXiv:1703.01310}, 2017.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In {\em NIPS-W}, 2017.

\bibitem{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A. Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em International Conference on Machine Learning}, volume 2017,
  2017.

\bibitem{Peters2010RelativeEP}
Jan Peters, Katharina Mulling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In {\em Proc. AAAI}, 2010.

\bibitem{peters2010relative}
Jan Peters, Katharina M{\"u}lling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In {\em AAAI}, pages 1607--1612. Atlanta, 2010.

\bibitem{rasmussen2005gaussian}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock {\em Gaussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press, 2005.

\bibitem{riedmiller2005neural}
Martin Riedmiller.
\newblock Neural fitted q iteration.
\newblock In {\em European Conference on Machine Learning}, pages 317--328.
  Springer, 2005.

\bibitem{robert2013monte}
Christian Robert and George Casella.
\newblock {\em Monte Carlo statistical methods}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{schaarschmidt2017tensorforce}
Michael Schaarschmidt, Alexander Kuhnle, and Kai Fricke.
\newblock Tensorforce: A tensorflow library for applied reinforcement learning.
\newblock Web page, 2017.

\bibitem{schmidhuber1991possibility}
J{\"u}rgen Schmidhuber.
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In {\em International Conference on Simulation of Adaptive Behavior:
  From animals to animats}, pages 222--227, 1991.

\bibitem{schmidhuber2008driven}
J{\"u}rgen Schmidhuber.
\newblock Driven by compression progress: A simple principle explains essential
  aspects of subjective beauty, novelty, surprise, interestingness, attention,
  curiosity, creativity, art, science, music, jokes.
\newblock In {\em Workshop on Anticipatory Behavior in Adaptive Learning
  Systems}, pages 48--76. Springer, 2008.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{schweighofer2003meta}
Nicolas Schweighofer and Kenji Doya.
\newblock Meta-learning in reinforcement learning.
\newblock {\em Neural Networks}, 16(1):5--9, 2003.

\bibitem{scott2010modern}
Steven~L. Scott.
\newblock A modern bayesian look at the multi-armed bandit.
\newblock {\em Applied Stochastic Models in Business and Industry},
  26(6):639--658, 2010.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J. Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{silver2017chess}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}, 2017.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354, 2017.

\bibitem{singh2004intrinsically}
Satinder~P. Singh, Andrew~G. Barto, and Nuttapong Chentanez.
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1281--1288, 2004.

\bibitem{smith2006optimizer}
James~E. Smith and Robert~L. Winkler.
\newblock The optimizer's curse: Skepticism and postdecision surprise in
  decision analysis.
\newblock {\em Management Science}, 52(3):311--322, 2006.

\bibitem{strehl2006pac}
Alexander~L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael~L.
  Littman.
\newblock Pac model-free reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  881--888. ACM, 2006.

\bibitem{strens2000bayesian}
Malcolm Strens.
\newblock A bayesian framework for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  943--950, 2000.

\bibitem{sutton1998reinforcement}
Richard~S. Sutton, Andrew~G. Barto, et~al.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 1998.

\bibitem{szita2008many}
Istv{\'a}n Szita and Andr{\'a}s L{\H{o}}rincz.
\newblock The many faces of optimism: a unifying approach.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 1048--1055. ACM, 2008.

\bibitem{simpledqn}
{Tambet, Matiisen}.
\newblock {simple-dqn}.
\newblock \url{https://github.com/tambetm/simple_dqn}, 2015.

\bibitem{tang2017exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI~Xi Chen, Yan
  Duan, John Schulman, Filip DeTurck, and Pieter Abbeel.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{tanner2009rl}
Brian Tanner and Adam White.
\newblock Rl-glue: Language-independent software for reinforcement-learning
  experiments.
\newblock {\em Journal of Machine Learning Research}, 10(Sep):2133--2136, 2009.

\bibitem{teh2017distral}
Yee Teh, Victor Bapst, Wojciech~M Czarnecki, John Quan, James Kirkpatrick, Raia
  Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4496--4506, 2017.

\bibitem{Tewari2007}
Ambuj Tewari and Peter~L. Bartlett.
\newblock {\em Bounded Parameter Markov Decision Processes with Average Reward
  Criterion}, pages 263--277.
\newblock 2007.

\bibitem{thompson1933likelihood}
William~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3/4):285--294, 1933.

\bibitem{van2010double}
Hado Van~Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2613--2621, 2010.

\bibitem{van2013estimating}
Hado Van~Hasselt.
\newblock Estimating the maximum expected value: an analysis of (nested)
  cross-validation and the maximum sample average.
\newblock {\em arXiv preprint arXiv:1302.7175}, 2013.

\bibitem{van2015deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock {\em CoRR}, abs/1509.06461, 2015.

\bibitem{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em AAAI}, volume~2, page~5. Phoenix, AZ, 2016.

\bibitem{vermorel2005multi}
Joannes Vermorel and Mehryar Mohri.
\newblock Multi-armed bandit algorithms and empirical evaluation.
\newblock In {\em European conference on machine learning}, pages 437--448.
  Springer, 2005.

\bibitem{vlassis2012bayesian}
Nikos Vlassis, Mohammad Ghavamzadeh, Shie Mannor, and Pascal Poupart.
\newblock Bayesian reinforcement learning.
\newblock In {\em Reinforcement Learning}, pages 359--386. Springer, 2012.

\bibitem{wang2015dueling}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van~Hasselt, Marc Lanctot, and Nando
  De~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1511.06581}, 2015.

\bibitem{watkins1989learning}
Christopher John Cornish~Hellaby Watkins.
\newblock {\em Learning from delayed rewards}.
\newblock PhD thesis, University of Cambridge England, 1989.

\bibitem{white2010interval}
Martha White and Adam White.
\newblock Interval estimation for reinforcement-learning algorithms in
  continuous-state domains.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2433--2441, 2010.

\bibitem{xu2013mab}
Min Xu, Tao Qin, and Tie yan Liu.
\newblock Estimation bias in multi-armed bandit algorithms for search
  advertising.
\newblock In Burges C.j.c., Bottou L., Welling M., Ghahramani Z., and
  Weinberger K.q., editors, {\em Advances in Neural Information Processing
  Systems 26}, pages 2400--2408. 2013.

\bibitem{yoshida2013reinforcement}
Naoto Yoshida, Eiji Uchibe, and Kenji Doya.
\newblock Reinforcement learning with state-dependent discount factor.
\newblock In {\em Proc. ICDL}, pages 1--6. IEEE, 2013.

\bibitem{yuan2017adversarial}
Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra~Rana Bhat, and Xiaolin Li.
\newblock Adversarial examples: Attacks and defenses for deep learning.
\newblock {\em arXiv preprint arXiv:1712.07107}, 2017.

\bibitem{ijcai2017-483}
Zhang Zongzhang, Pan Zhiyuan, and Kochenderfer Mykel~J.
\newblock Weighted double q-learning.
\newblock In {\em Proceedings of the Twenty-Sixth International Joint
  Conference on Artificial Intelligence, {IJCAI-17}}, pages 3455--3461, 2017.

\end{thebibliography}
