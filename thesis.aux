\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\select@language{greek}
\@writefile{toc}{\select@language{greek}}
\@writefile{lof}{\select@language{greek}}
\@writefile{lot}{\select@language{greek}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\gdef \LT@i {\LT@entry 
    {1}{39.33574pt}\LT@entry 
    {1}{219.0021pt}}
\@writefile{toc}{\contentsline {chapter}{Glossary}{\textlatin  {XIII}}{section*.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{C:intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Perception and interaction}{1}{section.1.1}}
\citation{sutton1998reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Reinforcement Learning problem scheme}}{2}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{F:rl}{{1.1}{2}{Reinforcement Learning problem scheme}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Learn how to act with Reinforcement Learning}{2}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Uncertainty in Reinforcement Learning}{2}{subsection.1.2.1}}
\citation{lai1985asymptotically}
\citation{bubeck2012regret,agrawal2012analysis,vermorel2005multi}
\citation{auer2002finite}
\citation{thompson1933likelihood}
\citation{mnih2015human,van2016deep,wang2015dueling}
\citation{silver2016mastering,silver2017mastering}
\citation{silver2017chess}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Balancing exploration and exploitation}{3}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}My research}{3}{section.1.3}}
\citation{mnih2015human}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Agent and environment}{5}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Reinforcement Learning problem scheme}}{5}{figure.caption.11}}
\newlabel{F:mdp1}{{2.1}{5}{Reinforcement Learning problem scheme}{figure.caption.11}{}}
\newlabel{E:sumrew}{{2.1}{6}{Agent and environment}{equation.2.1.1}{}}
\newlabel{E:discumrew}{{2.3}{6}{Agent and environment}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{6}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Markov Decision Process}}{7}{figure.caption.12}}
\newlabel{F:mdp2}{{2.2}{7}{Markov Decision Process}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Value functions}{7}{subsection.2.2.1}}
\citation{bertsekas2005dynamic,bellman2013dynamic}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Solving a MDP}{8}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Dynamic Programming}{8}{subsection.2.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Iterative Policy Evaluation\relax }}{9}{algorithm.1}}
\newlabel{A:peval}{{1}{9}{Iterative Policy Evaluation\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Value Iteration\relax }}{9}{algorithm.2}}
\newlabel{A:piter}{{2}{9}{Value Iteration\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Policy Iteration}{9}{subsubsection*.13}}
\@writefile{toc}{\contentsline {subsubsection}{Value Iteration}{10}{subsubsection*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Reinforcement Learning}{10}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Online}{10}{subsubsection*.15}}
\@writefile{toc}{\contentsline {subsubsection}{Batch}{10}{subsubsection*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Deep Reinforcement Learning}{10}{subsubsection*.17}}
\citation{van2010double}
\citation{smith2006optimizer}
\citation{van2010double}
\citation{van2013estimating}
\citation{van2013estimating}
\citation{deramo2016estimating}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Maximum Expected Value estimation}{11}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{deramo2017maximum}
\citation{rasmussen2005gaussian}
\citation{smith2006optimizer}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problem definition}{12}{section.3.1}}
\newlabel{E:maxExp}{{3.1}{12}{Problem definition}{equation.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Related Works}{12}{subsection.3.1.1}}
\newlabel{E:biasME}{{3.2}{12}{Related Works}{equation.3.1.2}{}}
\citation{van2010double}
\citation{van2013estimating}
\citation{van2010double}
\citation{xu2013mab}
\newlabel{E:biasCV}{{3.3}{13}{Related Works}{equation.3.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Weighted Estimator}{13}{section.3.2}}
\newlabel{E:WE}{{3.4}{13}{Weighted Estimator}{equation.3.2.4}{}}
\newlabel{E:OptimalWE}{{3.5}{14}{Weighted Estimator}{equation.3.2.5}{}}
\newlabel{E:WE2}{{3.6}{14}{Weighted Estimator}{equation.3.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Generalization to Infinite Random Variables}{14}{subsection.3.2.1}}
\newlabel{S: infinite}{{3.2.1}{14}{Generalization to Infinite Random Variables}{subsection.3.2.1}{}}
\newlabel{E:continuousWE}{{3.7}{14}{Generalization to Infinite Random Variables}{equation.3.2.7}{}}
\citation{grossman1972non}
\newlabel{E:probability_events}{{3.8}{15}{Generalization to Infinite Random Variables}{equation.3.2.8}{}}
\newlabel{E:probability_division}{{3.9}{15}{Generalization to Infinite Random Variables}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Spatially Correlated Variables}{15}{subsubsection*.18}}
\newlabel{E:continuousWE2}{{3.10}{15}{Spatially Correlated Variables}{equation.3.2.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Process Regression}{15}{subsubsection*.19}}
\citation{rasmussen2005gaussian}
\citation{watkins1989learning}
\citation{watkins1989learning}
\citation{lee2013bias,bellemare2015increasing,ijcai2017-483}
\citation{van2010double}
\citation{deramo2016estimating}
\newlabel{E:gpmean}{{3.11}{16}{Gaussian Process Regression}{equation.3.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Maximum Expected Value estimation in Reinforcement Learning}{16}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Online}{16}{subsection.3.3.1}}
\newlabel{eq:Q-formula}{{3.12}{16}{Online}{equation.3.3.12}{}}
\citation{Ernst2005tree}
\citation{deramo2017maximum}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Weighted Q-learning\relax }}{17}{algorithm.3}}
\newlabel{A:WQ-Learning}{{3}{17}{Weighted Q-learning\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Weighted Q-Learning}{17}{subsubsection*.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Batch}{17}{subsection.3.3.2}}
\citation{mnih2015human}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Weighted FQI (finite actions)\relax }}{18}{algorithm.4}}
\newlabel{A:WFQI}{{4}{18}{Weighted FQI (finite actions)\relax }{algorithm.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Weighted FQI$_{\infty }$ (continuous actions)\relax }}{18}{algorithm.5}}
\newlabel{A:continuousWFQI}{{5}{18}{Weighted FQI$_{\infty }$ (continuous actions)\relax }{algorithm.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Weighted Fitted Q-Iteration}{18}{paragraph*.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Deep Reinforcement Learning}{18}{subsection.3.3.3}}
\newlabel{S:WDQN}{{3.3.3}{18}{Deep Reinforcement Learning}{subsection.3.3.3}{}}
\citation{hasselt2015double}
\citation{osband2017deep}
\citation{mnih2015human}
\citation{van2013estimating}
\@writefile{toc}{\contentsline {paragraph}{Weighted Deep Q-Network}{19}{paragraph*.22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Empirical results}{19}{section.3.4}}
\newlabel{F:ia_first}{{3.1(a)}{20}{Subfigure 3 3.1(a)}{subfigure.3.1.1}{}}
\newlabel{sub@F:ia_first}{{(a)}{20}{Subfigure 3 3.1(a)\relax }{subfigure.3.1.1}{}}
\newlabel{F:ia_second}{{3.1(b)}{20}{Subfigure 3 3.1(b)}{subfigure.3.1.2}{}}
\newlabel{sub@F:ia_second}{{(b)}{20}{Subfigure 3 3.1(b)\relax }{subfigure.3.1.2}{}}
\newlabel{F:ia_third}{{3.1(c)}{20}{Subfigure 3 3.1(c)}{subfigure.3.1.3}{}}
\newlabel{sub@F:ia_third}{{(c)}{20}{Subfigure 3 3.1(c)\relax }{subfigure.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces MSE for each setting. Results are averaged over 2000 experiments.\relax }}{20}{figure.caption.24}}
\newlabel{F:iAds}{{3.1}{20}{MSE for each setting. Results are averaged over 2000 experiments.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Increasing number of impressions.}}}{20}{figure.caption.24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Increasing number of ads.}}}{20}{figure.caption.24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Increasing value of maximum CTR.}}}{20}{figure.caption.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Discrete States and Action Spaces}{20}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{Internet Ads}{20}{subsubsection*.23}}
\citation{xu2013mab}
\citation{auer2002finite}
\citation{xu2013mab}
\citation{xu2013mab}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Relative player 1 utility gain for different value of the bid defined as $\frac  {utility(b)}{utility(v)} - 1$. Results are averaged over 2000 experiments.\relax }}{21}{figure.caption.25}}
\newlabel{F:spSearch}{{3.2}{21}{Relative player 1 utility gain for different value of the bid defined as $\frac {utility(b)}{utility(v)} - 1$. Results are averaged over 2000 experiments.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sponsored Search Auctions}{21}{subsubsection*.26}}
\citation{van2010double}
\citation{lee2012intelligent,lee2013bias}
\@writefile{toc}{\contentsline {subsubsection}{Grid World}{22}{subsubsection*.27}}
\@writefile{toc}{\contentsline {subsubsection}{Forex}{22}{subsubsection*.30}}
\newlabel{F:bernoulli}{{3.3(a)}{23}{Subfigure 3 3.3(a)}{subfigure.3.3.1}{}}
\newlabel{sub@F:bernoulli}{{(a)}{23}{Subfigure 3 3.3(a)\relax }{subfigure.3.3.1}{}}
\newlabel{F:gaussian5}{{3.3(b)}{23}{Subfigure 3 3.3(b)}{subfigure.3.3.2}{}}
\newlabel{sub@F:gaussian5}{{(b)}{23}{Subfigure 3 3.3(b)\relax }{subfigure.3.3.2}{}}
\newlabel{F:gaussian1}{{3.3(c)}{23}{Subfigure 3 3.3(c)}{subfigure.3.3.3}{}}
\newlabel{sub@F:gaussian1}{{(c)}{23}{Subfigure 3 3.3(c)\relax }{subfigure.3.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Grid world results with the three reward functions averaged over 10000 experiments. Optimal policy is the black line.\relax }}{23}{figure.caption.28}}
\newlabel{F:grid}{{3.3}{23}{Grid world results with the three reward functions averaged over 10000 experiments. Optimal policy is the black line.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Bernoulli.}}}{23}{figure.caption.28}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\mathcal {N}(-1, 5)$.}}}{23}{figure.caption.28}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\mathcal {N}(-1, 1)$.}}}{23}{figure.caption.28}}
\newlabel{F:forex_train}{{3.4(a)}{24}{Subfigure 3 3.4(a)}{subfigure.3.4.1}{}}
\newlabel{sub@F:forex_train}{{(a)}{24}{Subfigure 3 3.4(a)\relax }{subfigure.3.4.1}{}}
\newlabel{F:forex_test}{{3.4(b)}{24}{Subfigure 3 3.4(b)}{subfigure.3.4.2}{}}
\newlabel{sub@F:forex_test}{{(b)}{24}{Subfigure 3 3.4(b)\relax }{subfigure.3.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Profit per year averaged over 100 experiments.\relax }}{24}{figure.caption.29}}
\newlabel{F:forex}{{3.4}{24}{Profit per year averaged over 100 experiments.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training phase.}}}{24}{figure.caption.29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test phase.}}}{24}{figure.caption.29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Continuous state spaces}{25}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{Pricing Problem}{25}{subsubsection*.31}}
\citation{doya2000reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Mean bias obtained by ME, DE and $\text  {WE}_{\infty }$ with different sample sizes and bins (only for ME and DE). \relax }}{26}{figure.caption.32}}
\newlabel{F:pricing_bias}{{3.5}{26}{Mean bias obtained by ME, DE and $\text {WE}_{\infty }$ with different sample sizes and bins (only for ME and DE). \relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Swing-Up Pendulum}{26}{subsubsection*.34}}
\citation{gym}
\citation{osband2017deep}
\citation{osband2017deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Variance of the bias obtained by ME, DE and $\text  {WE}_{\infty }$ with different sample sizes and bins. \relax }}{27}{figure.caption.33}}
\newlabel{F:pricing_variance}{{3.6}{27}{Variance of the bias obtained by ME, DE and $\text {WE}_{\infty }$ with different sample sizes and bins. \relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Deep Reinforcement Learning Scenario}{27}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{Acrobot}{27}{subsubsection*.36}}
\newlabel{F:pendulum_discrete}{{3.7(a)}{28}{Subfigure 3 3.7(a)}{subfigure.3.7.1}{}}
\newlabel{sub@F:pendulum_discrete}{{(a)}{28}{Subfigure 3 3.7(a)\relax }{subfigure.3.7.1}{}}
\newlabel{F:pendulum_continuous}{{3.7(b)}{28}{Subfigure 3 3.7(b)}{subfigure.3.7.2}{}}
\newlabel{sub@F:pendulum_continuous}{{(b)}{28}{Subfigure 3 3.7(b)\relax }{subfigure.3.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Average reward averaged on 100 experiments.\relax }}{28}{figure.caption.35}}
\newlabel{F:forex}{{3.7}{28}{Average reward averaged on 100 experiments.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Discrete actions.}}}{28}{figure.caption.35}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Continuous actions.}}}{28}{figure.caption.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Average reward averaged on 10 experiments. \relax }}{29}{figure.caption.37}}
\newlabel{F:acrobot}{{3.8}{29}{Average reward averaged on 10 experiments. \relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Average reward in continuous action MDP.\relax }}{30}{table.caption.38}}
\newlabel{T:acrobot_pars}{{3.1}{30}{Average reward in continuous action MDP.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Exploration}{31}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Deep}{33}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Mushroom}{35}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{37}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{agrawal2012analysis}{1}
\bibcite{auer2002finite}{2}
\bibcite{bellemare2015increasing}{3}
\bibcite{bellman2013dynamic}{4}
\bibcite{bertsekas2005dynamic}{5}
\bibcite{bubeck2012regret}{6}
\bibcite{deramo2017maximum}{7}
\bibcite{deramo2016estimating}{8}
\bibcite{grossman1972non}{9}
\bibcite{hasselt2015double}{10}
\bibcite{lai1985asymptotically}{11}
\bibcite{lee2013bias}{12}
\bibcite{mnih2015human}{13}
\bibcite{osband2017deep}{14}
\bibcite{rasmussen2005gaussian}{15}
\bibcite{silver2016mastering}{16}
\bibcite{silver2017chess}{17}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{39}{section*.39}}
\bibcite{silver2017mastering}{18}
\bibcite{smith2006optimizer}{19}
\bibcite{sutton1998reinforcement}{20}
\bibcite{thompson1933likelihood}{21}
\bibcite{van2010double}{22}
\bibcite{van2013estimating}{23}
\bibcite{van2016deep}{24}
\bibcite{vermorel2005multi}{25}
\bibcite{wang2015dueling}{26}
\bibcite{watkins1989learning}{27}
\bibcite{xu2013mab}{28}
\bibcite{ijcai2017-483}{29}
