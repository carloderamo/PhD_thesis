\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\select@language{greek}
\@writefile{toc}{\select@language{greek}}
\@writefile{lof}{\select@language{greek}}
\@writefile{lot}{\select@language{greek}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{Glossary}{\textlatin  {XV}}{section*.8}}
\gdef \LT@i {\LT@entry 
    {3}{49.3198pt}\LT@entry 
    {1}{219.0021pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{C:intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Perception and interaction}{1}{section.1.1}}
\citation{sutton1998reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Reinforcement Learning problem scheme}}{2}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{F:rl}{{1.1}{2}{Reinforcement Learning problem scheme}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Learn how to act with Reinforcement Learning}{2}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Uncertainty in Reinforcement Learning}{2}{subsection.1.2.1}}
\citation{lai1985asymptotically}
\citation{bubeck2012regret,agrawal2012analysis,vermorel2005multi}
\citation{auer2002finite}
\citation{thompson1933likelihood}
\citation{mnih2015human,van2016deep,wang2015dueling}
\citation{silver2016mastering,silver2017mastering}
\citation{silver2017chess}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Balancing exploration and exploitation}{3}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}My research}{3}{section.1.3}}
\citation{mnih2015human}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Agent and environment}{5}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Reinforcement Learning problem scheme}}{5}{figure.caption.11}}
\newlabel{F:mdp1}{{2.1}{5}{Reinforcement Learning problem scheme}{figure.caption.11}{}}
\newlabel{E:sumrew}{{2.1}{6}{Agent and environment}{equation.2.1.1}{}}
\newlabel{E:discumrew}{{2.3}{6}{Agent and environment}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{6}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Markov Decision Process}}{7}{figure.caption.12}}
\newlabel{F:mdp2}{{2.2}{7}{Markov Decision Process}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Value functions}{7}{subsection.2.2.1}}
\citation{bertsekas2005dynamic,bellman2013dynamic}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Solving a MDP}{8}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Dynamic Programming}{8}{subsection.2.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Iterative Policy Evaluation\relax }}{9}{algorithm.1}}
\newlabel{A:peval}{{1}{9}{Iterative Policy Evaluation\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Value Iteration\relax }}{9}{algorithm.2}}
\newlabel{A:piter}{{2}{9}{Value Iteration\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Policy Iteration}{9}{subsubsection*.13}}
\@writefile{toc}{\contentsline {subsubsection}{Value Iteration}{10}{subsubsection*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Reinforcement Learning}{10}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Online}{10}{subsubsection*.15}}
\@writefile{toc}{\contentsline {subsubsection}{Batch}{10}{subsubsection*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Deep Reinforcement Learning}{10}{subsubsection*.17}}
\citation{van2010double}
\citation{smith2006optimizer}
\citation{van2010double}
\citation{van2013estimating}
\citation{van2013estimating}
\citation{deramo2016estimating}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Maximum Expected Value estimation}{11}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{deramo2017maximum}
\citation{smith2006optimizer}
\citation{van2010double}
\citation{van2013estimating}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problem definition}{12}{section.3.1}}
\newlabel{E:maxExp}{{3.1}{12}{Problem definition}{equation.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Related Works}{12}{subsection.3.1.1}}
\newlabel{E:biasME}{{3.2}{12}{Related Works}{equation.3.1.2}{}}
\citation{van2010double}
\citation{xu2013mab}
\newlabel{E:biasCV}{{3.3}{13}{Related Works}{equation.3.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Weighted Estimator}{13}{section.3.2}}
\newlabel{E:WE}{{3.4}{13}{Weighted Estimator}{equation.3.2.4}{}}
\newlabel{E:OptimalWE}{{3.5}{13}{Weighted Estimator}{equation.3.2.5}{}}
\newlabel{E:WE2}{{3.6}{14}{Weighted Estimator}{equation.3.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Generalization to Infinite Random Variables}{14}{subsection.3.2.1}}
\newlabel{S: infinite}{{3.2.1}{14}{Generalization to Infinite Random Variables}{subsection.3.2.1}{}}
\newlabel{E:continuousWE}{{3.7}{14}{Generalization to Infinite Random Variables}{equation.3.2.7}{}}
\citation{grossman1972non}
\newlabel{E:probability_events}{{3.8}{15}{Generalization to Infinite Random Variables}{equation.3.2.8}{}}
\newlabel{E:probability_division}{{3.9}{15}{Generalization to Infinite Random Variables}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Spatially Correlated Variables}{15}{subsubsection*.18}}
\newlabel{E:continuousWE2}{{3.10}{15}{Spatially Correlated Variables}{equation.3.2.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Process Regression}{15}{subsubsection*.19}}
\citation{rasmussen2005gaussian}
\citation{van2013estimating}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of the bias of the different estimators varying the difference of the means\relax }}{16}{figure.caption.20}}
\newlabel{F:bias}{{3.1}{16}{Comparison of the bias of the different estimators varying the difference of the means\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Comparison of the absolute bias of the different estimators varying the difference of the means.\relax }}{16}{figure.caption.20}}
\newlabel{F:absolute_bias}{{3.2}{16}{Comparison of the absolute bias of the different estimators varying the difference of the means.\relax }{figure.caption.20}{}}
\newlabel{E:gpmean}{{3.11}{16}{Gaussian Process Regression}{equation.3.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Analysis of Weighted Estimator}{16}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Bias}{16}{subsection.3.3.1}}
\citation{van2013estimating}
\newlabel{T:BiasWEME}{{1}{17}{}{theorem.1}{}}
\newlabel{T:BiasWECV}{{2}{17}{}{theorem.2}{}}
\citation{van2013estimating}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparison of the variance of the different estimators varying the difference of the means.\relax }}{18}{figure.caption.21}}
\newlabel{F:variance}{{3.3}{18}{Comparison of the variance of the different estimators varying the difference of the means.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Comparison of the MSE of the different estimators varying the difference of the means.\relax }}{18}{figure.caption.21}}
\newlabel{F:mse}{{3.4}{18}{Comparison of the MSE of the different estimators varying the difference of the means.\relax }{figure.caption.21}{}}
\newlabel{F:Variance_mse}{{\caption@xref {F:Variance_mse}{ on input line 201}}{18}{Variance}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Variance}{18}{subsection.3.3.2}}
\newlabel{T:VarianceWE}{{3}{18}{}{theorem.3}{}}
\newlabel{T:VarianceOWE}{{4}{18}{}{theorem.4}{}}
\citation{watkins1989learning}
\citation{watkins1989learning}
\citation{lee2013bias,bellemare2015increasing,ijcai2017-483}
\citation{van2010double}
\citation{deramo2016estimating}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Weighted Q-learning\relax }}{19}{algorithm.3}}
\newlabel{A:WQ-Learning}{{3}{19}{Weighted Q-learning\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Maximum Expected Value estimation in Reinforcement Learning}{19}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Online}{19}{subsection.3.4.1}}
\newlabel{eq:Q-formula}{{3.12}{19}{Online}{equation.3.4.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Weighted Q-Learning}{19}{subsubsection*.22}}
\citation{Ernst2005tree}
\citation{deramo2017maximum}
\citation{van2013estimating}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Batch}{20}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {paragraph}{Weighted Fitted Q-Iteration}{20}{paragraph*.23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Empirical results}{20}{section.3.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Weighted FQI (finite actions)\relax }}{21}{algorithm.4}}
\newlabel{A:WFQI}{{4}{21}{Weighted FQI (finite actions)\relax }{algorithm.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Weighted FQI$_{\infty }$ (continuous actions)\relax }}{21}{algorithm.5}}
\newlabel{A:continuousWFQI}{{5}{21}{Weighted FQI$_{\infty }$ (continuous actions)\relax }{algorithm.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Discrete States and Action Spaces}{21}{subsection.3.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{Internet Ads}{21}{subsubsection*.24}}
\citation{xu2013mab}
\citation{auer2002finite}
\citation{xu2013mab}
\newlabel{F:ia_first}{{3.5(a)}{22}{Subfigure 3 3.5(a)}{subfigure.3.5.1}{}}
\newlabel{sub@F:ia_first}{{(a)}{22}{Subfigure 3 3.5(a)\relax }{subfigure.3.5.1}{}}
\newlabel{F:ia_second}{{3.5(b)}{22}{Subfigure 3 3.5(b)}{subfigure.3.5.2}{}}
\newlabel{sub@F:ia_second}{{(b)}{22}{Subfigure 3 3.5(b)\relax }{subfigure.3.5.2}{}}
\newlabel{F:ia_third}{{3.5(c)}{22}{Subfigure 3 3.5(c)}{subfigure.3.5.3}{}}
\newlabel{sub@F:ia_third}{{(c)}{22}{Subfigure 3 3.5(c)\relax }{subfigure.3.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces MSE for each setting. Results are averaged over 2000 experiments.\relax }}{22}{figure.caption.25}}
\newlabel{F:iAds}{{3.5}{22}{MSE for each setting. Results are averaged over 2000 experiments.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Increasing number of impressions.}}}{22}{figure.caption.25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Increasing number of ads.}}}{22}{figure.caption.25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Increasing value of maximum CTR.}}}{22}{figure.caption.25}}
\@writefile{toc}{\contentsline {subsubsection}{Sponsored Search Auctions}{22}{subsubsection*.26}}
\citation{xu2013mab}
\citation{van2010double}
\citation{lee2012intelligent,lee2013bias}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Relative player 1 utility gain for different value of the bid defined as $\frac  {utility(b)}{utility(v)} - 1$. Results are averaged over 2000 experiments.\relax }}{23}{figure.caption.27}}
\newlabel{F:spSearch}{{3.6}{23}{Relative player 1 utility gain for different value of the bid defined as $\frac {utility(b)}{utility(v)} - 1$. Results are averaged over 2000 experiments.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Grid World}{23}{subsubsection*.28}}
\newlabel{F:bernoulli}{{3.7(a)}{24}{Subfigure 3 3.7(a)}{subfigure.3.7.1}{}}
\newlabel{sub@F:bernoulli}{{(a)}{24}{Subfigure 3 3.7(a)\relax }{subfigure.3.7.1}{}}
\newlabel{F:gaussian5}{{3.7(b)}{24}{Subfigure 3 3.7(b)}{subfigure.3.7.2}{}}
\newlabel{sub@F:gaussian5}{{(b)}{24}{Subfigure 3 3.7(b)\relax }{subfigure.3.7.2}{}}
\newlabel{F:gaussian1}{{3.7(c)}{24}{Subfigure 3 3.7(c)}{subfigure.3.7.3}{}}
\newlabel{sub@F:gaussian1}{{(c)}{24}{Subfigure 3 3.7(c)\relax }{subfigure.3.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Grid world results with the three reward functions averaged over 10000 experiments. Optimal policy is the black line.\relax }}{24}{figure.caption.29}}
\newlabel{F:grid}{{3.7}{24}{Grid world results with the three reward functions averaged over 10000 experiments. Optimal policy is the black line.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Bernoulli.}}}{24}{figure.caption.29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\mathcal {N}(-1, 5)$.}}}{24}{figure.caption.29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\mathcal {N}(-1, 1)$.}}}{24}{figure.caption.29}}
\newlabel{F:forex_train}{{3.8(a)}{25}{Subfigure 3 3.8(a)}{subfigure.3.8.1}{}}
\newlabel{sub@F:forex_train}{{(a)}{25}{Subfigure 3 3.8(a)\relax }{subfigure.3.8.1}{}}
\newlabel{F:forex_test}{{3.8(b)}{25}{Subfigure 3 3.8(b)}{subfigure.3.8.2}{}}
\newlabel{sub@F:forex_test}{{(b)}{25}{Subfigure 3 3.8(b)\relax }{subfigure.3.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Profit per year averaged over 100 experiments.\relax }}{25}{figure.caption.30}}
\newlabel{F:forex}{{3.8}{25}{Profit per year averaged over 100 experiments.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training phase.}}}{25}{figure.caption.30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test phase.}}}{25}{figure.caption.30}}
\@writefile{toc}{\contentsline {subsubsection}{Forex}{26}{subsubsection*.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Continuous state spaces}{26}{subsection.3.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Pricing Problem}{26}{subsubsection*.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Mean bias obtained by ME, DE and $\text  {WE}_{\infty }$ with different sample sizes and bins (only for ME and DE). \relax }}{27}{figure.caption.33}}
\newlabel{F:pricing_bias}{{3.9}{27}{Mean bias obtained by ME, DE and $\text {WE}_{\infty }$ with different sample sizes and bins (only for ME and DE). \relax }{figure.caption.33}{}}
\citation{doya2000reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Variance of the bias obtained by ME, DE and $\text  {WE}_{\infty }$ with different sample sizes and bins. \relax }}{28}{figure.caption.34}}
\newlabel{F:pricing_variance}{{3.10}{28}{Variance of the bias obtained by ME, DE and $\text {WE}_{\infty }$ with different sample sizes and bins. \relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{Swing-Up Pendulum}{28}{subsubsection*.35}}
\newlabel{F:pendulum_discrete}{{3.11(a)}{29}{Subfigure 3 3.11(a)}{subfigure.3.11.1}{}}
\newlabel{sub@F:pendulum_discrete}{{(a)}{29}{Subfigure 3 3.11(a)\relax }{subfigure.3.11.1}{}}
\newlabel{F:pendulum_continuous}{{3.11(b)}{29}{Subfigure 3 3.11(b)}{subfigure.3.11.2}{}}
\newlabel{sub@F:pendulum_continuous}{{(b)}{29}{Subfigure 3 3.11(b)\relax }{subfigure.3.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Average reward averaged on 100 experiments.\relax }}{29}{figure.caption.36}}
\newlabel{F:forex}{{3.11}{29}{Average reward averaged on 100 experiments.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Discrete actions.}}}{29}{figure.caption.36}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Continuous actions.}}}{29}{figure.caption.36}}
\citation{watkins1989learning}
\citation{smith2006optimizer,van2004rational}
\citation{van2010double}
\citation{van2013estimating}
\citation{van2016deep}
\citation{mnih2015human}
\citation{deramo2016estimating}
\citation{lee2013bias}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Exploiting uncertainty of the Bellman operator components to deal with highly stochastic problems}{31}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{NIPS2011_4251}
\citation{mohagheghi2007proportional,Tewari2007}
\citation{schweighofer2003meta,Kobayashi2009,yoshida2013reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Preliminaries}{32}{section.4.1}}
\newlabel{eq:qopt}{{4.1}{32}{Preliminaries}{equation.4.1.1}{}}
\newlabel{eq:qdec}{{4.2}{32}{Preliminaries}{equation.4.1.2}{}}
\newlabel{eq:rqtilde}{{4.3}{33}{Preliminaries}{equation.4.1.3}{}}
\newlabel{eq:qdecrqtilde}{{4.4}{33}{Preliminaries}{equation.4.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Proposed Method}{33}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Decomposition of the TD error}{33}{subsection.4.2.1}}
\newlabel{eq:rtilupdedate}{{4.5}{33}{Decomposition of the TD error}{equation.4.2.5}{}}
\newlabel{eq:qtildeupdate}{{4.6}{33}{Decomposition of the TD error}{equation.4.2.6}{}}
\citation{crites1996improving,bao2008infinite,franccois2015discount}
\newlabel{eq:cumulativeupdate}{{4.7}{34}{Decomposition of the TD error}{equation.4.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Analysis of the decomposed update}{34}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Variance dependent learning rate}{34}{subsection.4.2.3}}
\citation{EvenDar2001,watkins1992q}
\newlabel{eq:alpha_eq}{{4.14}{35}{Variance dependent learning rate}{equation.4.2.14}{}}
\newlabel{eq:delta_eq}{{4.15}{35}{Variance dependent learning rate}{equation.4.2.15}{}}
\citation{watkins1992q}
\citation{van2010double}
\citation{d2016estimating}
\citation{NIPS2011_4251}
\citation{van2010double}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Discussion on convergence}{36}{subsection.4.2.4}}
\newlabel{eq:lr_cond}{{4.16}{36}{Discussion on convergence}{equation.4.2.16}{}}
\newlabel{eq:alpha_smv}{{4.17}{36}{Discussion on convergence}{equation.4.2.17}{}}
\newlabel{eq:beta_delta_smv}{{4.20}{36}{Discussion on convergence}{equation.4.2.20}{}}
\citation{van2010double}
\citation{d2016estimating}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experimental Results}{37}{section.4.3}}
\newlabel{S:empirical}{{4.3}{37}{Experimental Results}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Noisy Grid World}{37}{subsection.4.3.1}}
\citation{Peters2010RelativeEP}
\newlabel{F:hasselt_all_1}{{4.1(a)}{38}{Subfigure 4 4.1(a)}{subfigure.4.1.1}{}}
\newlabel{sub@F:hasselt_all_1}{{(a)}{38}{Subfigure 4 4.1(a)\relax }{subfigure.4.1.1}{}}
\newlabel{F:hasselt_all_08}{{4.1(b)}{38}{Subfigure 4 4.1(b)}{subfigure.4.1.2}{}}
\newlabel{sub@F:hasselt_all_08}{{(b)}{38}{Subfigure 4 4.1(b)\relax }{subfigure.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of all the other algorithms and of the best setting of RQ-Learning for this experiment. Results are averaged over $10000$ experiments.\relax }}{38}{figure.caption.37}}
\newlabel{F:hasselt_all}{{4.1}{38}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of all the other algorithms and of the best setting of RQ-Learning for this experiment. Results are averaged over $10000$ experiments.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{38}{figure.caption.37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{38}{figure.caption.37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Double Chain}{38}{subsection.4.3.2}}
\newlabel{F:hasselt_qdec_1}{{4.2(a)}{39}{Subfigure 4 4.2(a)}{subfigure.4.2.1}{}}
\newlabel{sub@F:hasselt_qdec_1}{{(a)}{39}{Subfigure 4 4.2(a)\relax }{subfigure.4.2.1}{}}
\newlabel{F:hasselt_qdec_08}{{4.2(b)}{39}{Subfigure 4 4.2(b)}{subfigure.4.2.2}{}}
\newlabel{sub@F:hasselt_qdec_08}{{(b)}{39}{Subfigure 4 4.2(b)\relax }{subfigure.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of the best setting of RQ-Learning for this experiment together with other less effective setting of RQ-Learning. Results are averaged over $10000$ experiments.\relax }}{39}{figure.caption.38}}
\newlabel{F:hasselt_QDecs}{{4.2}{39}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of the best setting of RQ-Learning for this experiment together with other less effective setting of RQ-Learning. Results are averaged over $10000$ experiments.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{39}{figure.caption.38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{39}{figure.caption.38}}
\newlabel{F:hasselt_qdectol}{{4.3(a)}{40}{Subfigure 4 4.3(a)}{subfigure.4.3.1}{}}
\newlabel{sub@F:hasselt_qdectol}{{(a)}{40}{Subfigure 4 4.3(a)\relax }{subfigure.4.3.1}{}}
\newlabel{F:hasselt_qdecwintol}{{4.3(b)}{40}{Subfigure 4 4.3(b)}{subfigure.4.3.2}{}}
\newlabel{sub@F:hasselt_qdecwintol}{{(b)}{40}{Subfigure 4 4.3(b)\relax }{subfigure.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of RQ-Learning (Figure \ref  {F:hasselt_qdectol}) and windowed RQ-Learning (Figure \ref  {F:hasselt_qdecwintol}) with different values of $\eta $ and $k = 0.8$. Results are averaged over $10000$ experiments.\relax }}{40}{figure.caption.39}}
\newlabel{F:hasselt_QDecTol}{{4.3}{40}{Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of RQ-Learning (Figure \ref {F:hasselt_qdectol}) and windowed RQ-Learning (Figure \ref {F:hasselt_qdecwintol}) with different values of $\eta $ and $k = 0.8$. Results are averaged over $10000$ experiments.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RQ-Learning}}}{40}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Windowed RQ-Learning}}}{40}{figure.caption.39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Grid World with Holes}{40}{subsection.4.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}On-policy learning}{40}{subsection.4.3.4}}
\newlabel{F:double_chain_1_1}{{4.4(a)}{41}{Subfigure 4 4.4(a)}{subfigure.4.4.1}{}}
\newlabel{sub@F:double_chain_1_1}{{(a)}{41}{Subfigure 4 4.4(a)\relax }{subfigure.4.4.1}{}}
\newlabel{F:double_chain_1_51}{{4.4(b)}{41}{Subfigure 4 4.4(b)}{subfigure.4.4.2}{}}
\newlabel{sub@F:double_chain_1_51}{{(b)}{41}{Subfigure 4 4.4(b)\relax }{subfigure.4.4.2}{}}
\newlabel{F:double_chain_5_1}{{4.4(c)}{41}{Subfigure 4 4.4(c)}{subfigure.4.4.3}{}}
\newlabel{sub@F:double_chain_5_1}{{(c)}{41}{Subfigure 4 4.4(c)\relax }{subfigure.4.4.3}{}}
\newlabel{F:double_chain_5_51}{{4.4(d)}{41}{Subfigure 4 4.4(d)}{subfigure.4.4.4}{}}
\newlabel{sub@F:double_chain_5_51}{{(d)}{41}{Subfigure 4 4.4(d)\relax }{subfigure.4.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Maximum action-value estimate in the Double Chain problem in state $1$ (\ref  {F:double_chain_1_1}, \ref  {F:double_chain_1_51}) and state $5$ (\ref  {F:double_chain_5_1}, \ref  {F:double_chain_5_51}). Results are averaged over $500$ experiments.\relax }}{41}{figure.caption.40}}
\newlabel{F:double_chain_q}{{4.4}{41}{Maximum action-value estimate in the Double Chain problem in state $1$ (\ref {F:double_chain_1_1}, \ref {F:double_chain_1_51}) and state $5$ (\ref {F:double_chain_5_1}, \ref {F:double_chain_5_51}). Results are averaged over $500$ experiments.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{41}{figure.caption.40}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{41}{figure.caption.40}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{41}{figure.caption.40}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{41}{figure.caption.40}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{41}{section.4.4}}
\newlabel{F:lrs_1_1}{{4.5(a)}{42}{Subfigure 4 4.5(a)}{subfigure.4.5.1}{}}
\newlabel{sub@F:lrs_1_1}{{(a)}{42}{Subfigure 4 4.5(a)\relax }{subfigure.4.5.1}{}}
\newlabel{F:lrs_1_51}{{4.5(b)}{42}{Subfigure 4 4.5(b)}{subfigure.4.5.2}{}}
\newlabel{sub@F:lrs_1_51}{{(b)}{42}{Subfigure 4 4.5(b)\relax }{subfigure.4.5.2}{}}
\newlabel{F:lrs_5_1}{{4.5(c)}{42}{Subfigure 4 4.5(c)}{subfigure.4.5.3}{}}
\newlabel{sub@F:lrs_5_1}{{(c)}{42}{Subfigure 4 4.5(c)\relax }{subfigure.4.5.3}{}}
\newlabel{F:lrs_5_51}{{4.5(d)}{42}{Subfigure 4 4.5(d)}{subfigure.4.5.4}{}}
\newlabel{sub@F:lrs_5_51}{{(d)}{42}{Subfigure 4 4.5(d)\relax }{subfigure.4.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Learning rate of the two actions in the Double Chain problem in state $1$ (\ref  {F:lrs_1_1}, \ref  {F:lrs_1_51}) and state $5$ (\ref  {F:lrs_5_1}, \ref  {F:lrs_5_51}) for RQ-Learning with and without windowed variance estimation. Results are averaged over $500$ experiments.\relax }}{42}{figure.caption.41}}
\newlabel{F:double_chain_lr}{{4.5}{42}{Learning rate of the two actions in the Double Chain problem in state $1$ (\ref {F:lrs_1_1}, \ref {F:lrs_1_51}) and state $5$ (\ref {F:lrs_5_1}, \ref {F:lrs_5_51}) for RQ-Learning with and without windowed variance estimation. Results are averaged over $500$ experiments.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{42}{figure.caption.41}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{42}{figure.caption.41}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{42}{figure.caption.41}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{42}{figure.caption.41}}
\newlabel{F:max_a_1}{{4.6(a)}{43}{Subfigure 4 4.6(a)}{subfigure.4.6.1}{}}
\newlabel{sub@F:max_a_1}{{(a)}{43}{Subfigure 4 4.6(a)\relax }{subfigure.4.6.1}{}}
\newlabel{F:max_a_51}{{4.6(b)}{43}{Subfigure 4 4.6(b)}{subfigure.4.6.2}{}}
\newlabel{sub@F:max_a_51}{{(b)}{43}{Subfigure 4 4.6(b)\relax }{subfigure.4.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Action with maximum value in the Double Chain problem in state $1$ and state $9$ for Q-Learning and windowed RQ-Learning.\relax }}{43}{figure.caption.42}}
\newlabel{F:max_a}{{4.6}{43}{Action with maximum value in the Double Chain problem in state $1$ and state $9$ for Q-Learning and windowed RQ-Learning.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{43}{figure.caption.42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{43}{figure.caption.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Structure of the double-chain problem.\relax }}{43}{figure.caption.43}}
\newlabel{F:double-chain}{{4.7}{43}{Structure of the double-chain problem.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the Grid World with Holes problem in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning for this experiment. Results are averaged over $10000$ experiments.\relax }}{44}{figure.caption.44}}
\newlabel{F:hole}{{4.8}{44}{Mean reward per step (top) and maximum action-value estimate in the Grid World with Holes problem in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning for this experiment. Results are averaged over $10000$ experiments.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Structure of the Grid World with Holes problem.\relax }}{44}{figure.caption.45}}
\newlabel{F:grid_hole_map}{{4.9}{44}{Structure of the Grid World with Holes problem.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the Noisy Grid World problem in the initial state (bottom) of SARSA and of the on-policy windowed version of RQ-Learning for this experiment. Results are averaged over $1000$ experiments.\relax }}{45}{figure.caption.46}}
\newlabel{F:sarsa}{{4.10}{45}{Mean reward per step (top) and maximum action-value estimate in the Noisy Grid World problem in the initial state (bottom) of SARSA and of the on-policy windowed version of RQ-Learning for this experiment. Results are averaged over $1000$ experiments.\relax }{figure.caption.46}{}}
\citation{lai1985asymptotically}
\citation{jaksch2010near,kakade2003sample,kearns2002near}
\citation{thompson1933likelihood}
\citation{chapelle2011empirical,granmo2010solving,may2011simulation,scott2010modern}
\citation{chentanez2005intrinsically,schmidhuber1991possibility}
\citation{bellemare2016unifying,tang2017exploration}
\citation{pathak2017curiosity}
\citation{bonarini2006self}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Exploration}{47}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{dearden1998bayesian}
\citation{auer2007logarithmic}
\citation{jaksch2010near}
\citation{auer2002finite}
\citation{strehl2006pac}
\citation{kearns2002near}
\citation{brafman2002r}
\citation{dearden1999model,kolter2009near}
\citation{strens2000bayesian}
\citation{osband2013more}
\citation{dearden1998bayesian}
\citation{d2016estimating}
\citation{osband2016generalization}
\citation{osband2016deep}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Related work}{48}{section.5.1}}
\citation{dearden1998bayesian}
\citation{d2016estimating}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Thompson Sampling in value-based Reinforcement Learning}{49}{section.5.2}}
\newlabel{S:tsrl}{{5.2}{49}{Thompson Sampling in value-based Reinforcement Learning}{section.5.2}{}}
\newlabel{E:max_prob}{{5.1}{49}{Thompson Sampling in value-based Reinforcement Learning}{equation.5.2.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Standard mean and variance update\relax }}{50}{algorithm.6}}
\newlabel{A:var_1}{{6}{50}{Standard mean and variance update\relax }{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Mean and variance update using momentums\relax }}{50}{algorithm.7}}
\newlabel{A:var_2}{{7}{50}{Mean and variance update using momentums\relax }{algorithm.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Efficient uncertainty estimation}{50}{section.5.3}}
\newlabel{S:uncertainty}{{5.3}{50}{Efficient uncertainty estimation}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Online estimation}{50}{subsection.5.3.1}}
\citation{hoeffding1963probability}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces SARSA with online variance update\relax }}{51}{algorithm.8}}
\newlabel{A:sarsa_var}{{8}{51}{SARSA with online variance update\relax }{algorithm.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces SARSA with online variance update and Hoeffding upper bound\relax }}{51}{algorithm.9}}
\newlabel{A:sarsa_hoeff}{{9}{51}{SARSA with online variance update and Hoeffding upper bound\relax }{algorithm.9}{}}
\citation{tang2017exploration}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces SARSA with function approximation with variance update\relax }}{52}{algorithm.10}}
\newlabel{A:SARSA-apprx}{{10}{52}{SARSA with function approximation with variance update\relax }{algorithm.10}{}}
\newlabel{E:hoeff}{{5.4}{52}{Online estimation}{equation.5.3.4}{}}
\citation{doi:10.1162/089976600300015204}
\citation{osband2016deep}
\citation{mnih2015human}
\citation{osband2013more}
\citation{van2016deep}
\citation{hasselt2010double}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Bootstrapping}{53}{subsection.5.3.2}}
\@writefile{toc}{\contentsline {paragraph}{Thompson Sampling via Bootstrapping}{53}{paragraph*.47}}
\@writefile{toc}{\contentsline {paragraph}{Bootstrapped Q-Learning}{53}{paragraph*.48}}
\citation{pmlr-v70-asadi17a}
\citation{pmlr-v70-asadi17a}
\citation{hasselt2010double}
\citation{van2016deep}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces Bootstrapped DQN with Thompson Sampling\relax }}{54}{algorithm.11}}
\newlabel{A:tsboot}{{11}{54}{Bootstrapped DQN with Thompson Sampling\relax }{algorithm.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments}{54}{section.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Discrete state space}{54}{subsection.5.4.1}}
\newlabel{F:taxi_online}{{5.1(a)}{55}{Subfigure 5 5.1(a)}{subfigure.5.1.1}{}}
\newlabel{sub@F:taxi_online}{{(a)}{55}{Subfigure 5 5.1(a)\relax }{subfigure.5.1.1}{}}
\newlabel{F:taxi_grid}{{5.1(b)}{55}{Subfigure 5 5.1(b)}{subfigure.5.1.2}{}}
\newlabel{sub@F:taxi_grid}{{(b)}{55}{Subfigure 5 5.1(b)\relax }{subfigure.5.1.2}{}}
\newlabel{F:taxi_boot}{{5.1(c)}{55}{Subfigure 5 5.1(c)}{subfigure.5.1.3}{}}
\newlabel{sub@F:taxi_boot}{{(c)}{55}{Subfigure 5 5.1(c)\relax }{subfigure.5.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Results on the \textbf  {Taxi} problem. Figure~\ref  {F:taxi_online} shows the performance of the online variance estimation in terms of mean reward per step. The plots on the left show results using the setting of the policies parameters that is optimal in evaluation; the ones on the right consider parameters optimal in training. An epoch corresponds to $1000$ training steps. Figure~\ref  {F:taxi_grid} shows the structure of the grid where S is the initial position of the agent, P is a passenger and G is the goal. Figure~\ref  {F:taxi_boot} shows the performance during training of the bootstrapping approach in terms of mean reward per step.\relax }}{55}{figure.caption.49}}
\newlabel{F:taxi}{{5.1}{55}{Results on the \textbf {Taxi} problem. Figure~\ref {F:taxi_online} shows the performance of the online variance estimation in terms of mean reward per step. The plots on the left show results using the setting of the policies parameters that is optimal in evaluation; the ones on the right consider parameters optimal in training. An epoch corresponds to $1000$ training steps. Figure~\ref {F:taxi_grid} shows the structure of the grid where S is the initial position of the agent, P is a passenger and G is the goal. Figure~\ref {F:taxi_boot} shows the performance during training of the bootstrapping approach in terms of mean reward per step.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Online variance}}}{55}{figure.caption.49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Taxi grid}}}{55}{figure.caption.49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Bootstrapped variance}}}{55}{figure.caption.49}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Other results}{55}{section.5.5}}
\newlabel{A:results}{{5.5}{55}{Other results}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Continuous state space}{55}{subsection.5.5.1}}
\citation{bellemare13arcade}
\newlabel{F:mountain_car}{{5.2(a)}{56}{Subfigure 5 5.2(a)}{subfigure.5.2.1}{}}
\newlabel{sub@F:mountain_car}{{(a)}{56}{Subfigure 5 5.2(a)\relax }{subfigure.5.2.1}{}}
\newlabel{F:acrobot}{{5.2(b)}{56}{Subfigure 5 5.2(b)}{subfigure.5.2.2}{}}
\newlabel{sub@F:acrobot}{{(b)}{56}{Subfigure 5 5.2(b)\relax }{subfigure.5.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Figure~\ref  {F:mountain_car} shows the mean cumulative reward in train and evaluation for Mountain Car, while Figure~\ref  {F:acrobot} shows the mean cumulative reward in evaluation for Acrobot. Evaluation epochs are performed every $20$ training episodes for the former, and every $1000$ training steps for the latter.\relax }}{56}{figure.caption.50}}
\newlabel{F:TS_cont}{{5.2}{56}{Figure~\ref {F:mountain_car} shows the mean cumulative reward in train and evaluation for Mountain Car, while Figure~\ref {F:acrobot} shows the mean cumulative reward in evaluation for Acrobot. Evaluation epochs are performed every $20$ training episodes for the former, and every $1000$ training steps for the latter.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Mountain Car}}}{56}{figure.caption.50}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Acrobot}}}{56}{figure.caption.50}}
\newlabel{F:pong}{{5.3(a)}{56}{Subfigure 5 5.3(a)}{subfigure.5.3.1}{}}
\newlabel{sub@F:pong}{{(a)}{56}{Subfigure 5 5.3(a)\relax }{subfigure.5.3.1}{}}
\newlabel{F:breakout}{{5.3(b)}{56}{Subfigure 5 5.3(b)}{subfigure.5.3.2}{}}
\newlabel{sub@F:breakout}{{(b)}{56}{Subfigure 5 5.3(b)\relax }{subfigure.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Mean cumulative reward in evaluation. An epoch is performed every $250000$ steps.\relax }}{56}{figure.caption.51}}
\newlabel{F:atari}{{5.3}{56}{Mean cumulative reward in evaluation. An epoch is performed every $250000$ steps.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Pong}}}{56}{figure.caption.51}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Breakout}}}{56}{figure.caption.51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Deep Reinforcement Learning}{56}{subsection.5.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Optimal parameter settings search during evaluation in Taxi.\relax }}{57}{figure.caption.52}}
\newlabel{F:taxisearch_eval}{{5.4}{57}{Optimal parameter settings search during evaluation in Taxi.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Optimal parameter settings search during training in Taxi.\relax }}{57}{figure.caption.53}}
\newlabel{F:taxisearch_train}{{5.5}{57}{Optimal parameter settings search during training in Taxi.\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Optimal parameter settings search during training in Mountain Car.\relax }}{57}{figure.caption.54}}
\newlabel{F:mountain_car_pars}{{5.6}{57}{Optimal parameter settings search during training in Mountain Car.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Results of TS on Taxi with different upper bounds on variance.\relax }}{57}{figure.caption.55}}
\newlabel{F:bounds}{{5.7}{57}{Results of TS on Taxi with different upper bounds on variance.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Results of TS on Taxi using Hoeffding upper bound with different values of the $c$ parameter.\relax }}{58}{figure.caption.56}}
\newlabel{F:cs}{{5.8}{58}{Results of TS on Taxi using Hoeffding upper bound with different values of the $c$ parameter.\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Results of TS on Taxi using Hoeffding upper bound with $c=2$ without and with $\chi ^2$ upper bound boost.\relax }}{58}{figure.caption.57}}
\newlabel{F:chics}{{5.9}{58}{Results of TS on Taxi using Hoeffding upper bound with $c=2$ without and with $\chi ^2$ upper bound boost.\relax }{figure.caption.57}{}}
\citation{mnih2015human}
\citation{hasselt2015double}
\citation{osband2017deep}
\citation{mnih2015human}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Deep}{59}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}Deep Reinforcement Learning}{59}{subsection.6.0.1}}
\newlabel{S:WDQN}{{6.0.1}{59}{Deep Reinforcement Learning}{subsection.6.0.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Weighted Deep Q-Network}{59}{paragraph*.58}}
\citation{gym}
\citation{osband2017deep}
\citation{osband2017deep}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Average reward in continuous action MDP.\relax }}{60}{table.caption.61}}
\newlabel{T:acrobot_pars}{{6.1}{60}{Average reward in continuous action MDP.\relax }{table.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.2}Deep Reinforcement Learning Scenario}{60}{subsection.6.0.2}}
\@writefile{toc}{\contentsline {subsubsection}{Acrobot}{60}{subsubsection*.59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Average reward averaged on 10 experiments. \relax }}{61}{figure.caption.60}}
\newlabel{F:acrobot}{{6.1}{61}{Average reward averaged on 10 experiments. \relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Mushroom}{63}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusion}{65}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{agrawal2012analysis}{1}
\bibcite{pmlr-v70-asadi17a}{2}
\bibcite{auer2002finite}{3}
\bibcite{auer2007logarithmic}{4}
\bibcite{bellemare13arcade}{5}
\bibcite{bellemare2016unifying}{6}
\bibcite{bellemare2015increasing}{7}
\bibcite{bellman2013dynamic}{8}
\bibcite{bertsekas2005dynamic}{9}
\bibcite{bonarini2006self}{10}
\bibcite{brafman2002r}{11}
\bibcite{gym}{12}
\bibcite{bubeck2012regret}{13}
\bibcite{chapelle2011empirical}{14}
\bibcite{chentanez2005intrinsically}{15}
\bibcite{dearden1999model}{16}
\bibcite{dearden1998bayesian}{17}
\bibcite{deramo2017maximum}{18}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{67}{section*.62}}
\bibcite{deramo2016estimating}{19}
\bibcite{doi:10.1162/089976600300015204}{20}
\bibcite{NIPS2011_4251}{21}
\bibcite{granmo2010solving}{22}
\bibcite{grossman1972non}{23}
\bibcite{hasselt2015double}{24}
\bibcite{hoeffding1963probability}{25}
\bibcite{jaksch2010near}{26}
\bibcite{kakade2003sample}{27}
\bibcite{kearns2002near}{28}
\bibcite{Kobayashi2009}{29}
\bibcite{kolter2009near}{30}
\bibcite{lai1985asymptotically}{31}
\bibcite{lee2013bias}{32}
\bibcite{may2011simulation}{33}
\bibcite{mnih2015human}{34}
\bibcite{mohagheghi2007proportional}{35}
\bibcite{osband2017deep}{36}
\bibcite{osband2013more}{37}
\bibcite{osband2016generalization}{38}
\bibcite{pathak2017curiosity}{39}
\bibcite{rasmussen2005gaussian}{40}
\bibcite{schmidhuber1991possibility}{41}
\bibcite{schweighofer2003meta}{42}
\bibcite{scott2010modern}{43}
\bibcite{silver2016mastering}{44}
\bibcite{silver2017chess}{45}
\bibcite{silver2017mastering}{46}
\bibcite{smith2006optimizer}{47}
\bibcite{strehl2006pac}{48}
\bibcite{strens2000bayesian}{49}
\bibcite{sutton1998reinforcement}{50}
\bibcite{tang2017exploration}{51}
\bibcite{Tewari2007}{52}
\bibcite{thompson1933likelihood}{53}
\bibcite{van2004rational}{54}
\bibcite{van2010double}{55}
\bibcite{van2013estimating}{56}
\bibcite{van2016deep}{57}
\bibcite{vermorel2005multi}{58}
\bibcite{wang2015dueling}{59}
\bibcite{watkins1989learning}{60}
\bibcite{xu2013mab}{61}
\bibcite{yoshida2013reinforcement}{62}
\bibcite{ijcai2017-483}{63}
