\select@language {greek}
\select@language {english}
\select@language {english}
\select@language {english}
\contentsline {chapter}{Glossary}{\textlatin {XI}}{section*.7}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Perception and interaction}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Learn how to act with Reinforcement Learning}{2}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Uncertainty in Reinforcement Learning}{2}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Balancing exploration and exploitation}{3}{subsection.1.2.2}
\contentsline {section}{\numberline {1.3}My research}{3}{section.1.3}
\contentsline {chapter}{\numberline {2}Preliminaries}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Agent and environment}{5}{section.2.1}
\contentsline {section}{\numberline {2.2}Markov Decision Processes}{6}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Value functions}{7}{subsection.2.2.1}
\contentsline {section}{\numberline {2.3}Solving a MDP}{8}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Dynamic Programming}{8}{subsection.2.3.1}
\contentsline {subsubsection}{Policy Iteration}{9}{subsubsection*.12}
\contentsline {subsubsection}{Value Iteration}{10}{subsubsection*.13}
\contentsline {subsection}{\numberline {2.3.2}Reinforcement Learning}{10}{subsection.2.3.2}
\contentsline {paragraph}{Exploratory policies}{11}{paragraph*.14}
\contentsline {subsubsection}{Online}{12}{subsubsection*.15}
\contentsline {paragraph}{SARSA}{12}{paragraph*.16}
\contentsline {paragraph}{Q-Learning}{12}{paragraph*.17}
\contentsline {paragraph}{Double Q-Learning}{12}{paragraph*.18}
\contentsline {subsubsection}{Batch}{12}{subsubsection*.19}
\contentsline {paragraph}{Fitted Q-Iteration}{12}{paragraph*.20}
\contentsline {subsubsection}{Deep Reinforcement Learning}{12}{subsubsection*.21}
\contentsline {chapter}{\numberline {3}Maximum Expected Value estimation}{13}{chapter.3}
\contentsline {section}{\numberline {3.1}Problem definition}{14}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Related Works}{14}{subsection.3.1.1}
\contentsline {section}{\numberline {3.2}Weighted Estimator}{15}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Generalization to Infinite Random Variables}{16}{subsection.3.2.1}
\contentsline {subsubsection}{Spatially Correlated Variables}{17}{subsubsection*.22}
\contentsline {subsubsection}{Gaussian Process Regression}{17}{subsubsection*.23}
\contentsline {section}{\numberline {3.3}Analysis of Weighted Estimator}{18}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Bias}{18}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Variance}{20}{subsection.3.3.2}
\contentsline {section}{\numberline {3.4}Maximum Expected Value estimation in Reinforcement Learning}{21}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Online}{21}{subsection.3.4.1}
\contentsline {subsubsection}{Weighted Q-Learning}{21}{subsubsection*.26}
\contentsline {subsection}{\numberline {3.4.2}Batch}{22}{subsection.3.4.2}
\contentsline {subsubsection}{Weighted Fitted Q-Iteration}{22}{subsubsection*.27}
\contentsline {section}{\numberline {3.5}Empirical results}{23}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Discrete States and Action Spaces}{23}{subsection.3.5.1}
\contentsline {subsubsection}{Internet Ads}{23}{subsubsection*.28}
\contentsline {subsubsection}{Sponsored Search Auctions}{24}{subsubsection*.30}
\contentsline {subsubsection}{Grid World}{25}{subsubsection*.32}
\contentsline {subsubsection}{Forex}{26}{subsubsection*.35}
\contentsline {subsection}{\numberline {3.5.2}Continuous state spaces}{27}{subsection.3.5.2}
\contentsline {subsubsection}{Pricing Problem}{27}{subsubsection*.36}
\contentsline {subsubsection}{Swing-Up Pendulum}{29}{subsubsection*.39}
\contentsline {chapter}{\numberline {4}Exploiting uncertainty of the Bellman operator components to deal with highly stochastic problems}{31}{chapter.4}
\contentsline {section}{\numberline {4.1}Preliminaries}{32}{section.4.1}
\contentsline {section}{\numberline {4.2}The Proposed Method}{33}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Decomposition of the TD error}{33}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Analysis of the decomposed update}{34}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Variance dependent learning rate}{34}{subsection.4.2.3}
\contentsline {subsection}{\numberline {4.2.4}Discussion on convergence}{36}{subsection.4.2.4}
\contentsline {section}{\numberline {4.3}Experimental Results}{37}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Noisy Grid World}{38}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Double Chain}{39}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Grid World with Holes}{41}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}On-policy learning}{42}{subsection.4.3.4}
\contentsline {chapter}{\numberline {5}Exploration}{45}{chapter.5}
\contentsline {section}{\numberline {5.1}Related work}{46}{section.5.1}
\contentsline {section}{\numberline {5.2}Thompson Sampling in value-based Reinforcement Learning}{47}{section.5.2}
\contentsline {section}{\numberline {5.3}Efficient uncertainty estimation}{48}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Online estimation}{48}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Bootstrapping}{51}{subsection.5.3.2}
\contentsline {paragraph}{Thompson Sampling via Bootstrapping}{51}{paragraph*.51}
\contentsline {paragraph}{Bootstrapped Q-Learning}{51}{paragraph*.52}
\contentsline {section}{\numberline {5.4}Experiments}{52}{section.5.4}
\contentsline {subsection}{\numberline {5.4.1}Discrete state space}{52}{subsection.5.4.1}
\contentsline {section}{\numberline {5.5}Other results}{53}{section.5.5}
\contentsline {subsection}{\numberline {5.5.1}Continuous state space}{53}{subsection.5.5.1}
\contentsline {subsection}{\numberline {5.5.2}Deep Reinforcement Learning}{54}{subsection.5.5.2}
\contentsline {chapter}{\numberline {6}Deep}{57}{chapter.6}
\contentsline {subsection}{\numberline {6.0.1}Deep Reinforcement Learning}{57}{subsection.6.0.1}
\contentsline {paragraph}{Weighted Deep Q-Network}{57}{paragraph*.59}
\contentsline {subsection}{\numberline {6.0.2}Deep Reinforcement Learning Scenario}{58}{subsection.6.0.2}
\contentsline {subsubsection}{Acrobot}{58}{subsubsection*.60}
\contentsline {chapter}{\numberline {7}Mushroom}{61}{chapter.7}
\contentsline {chapter}{\numberline {8}Conclusion}{63}{chapter.8}
\contentsline {chapter}{Bibliography}{65}{section*.63}
