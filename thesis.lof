\select@language {greek}
\select@language {english}
\select@language {english}
\select@language {english}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Reinforcement Learning problem scheme}}{2}{figure.caption.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Reinforcement Learning problem scheme}}{5}{figure.caption.11}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Markov Decision Process}}{7}{figure.caption.12}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of the bias of the different estimators varying the difference of the means\relax }}{16}{figure.caption.20}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Comparison of the absolute bias of the different estimators varying the difference of the means.\relax }}{16}{figure.caption.20}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparison of the variance of the different estimators varying the difference of the means.\relax }}{18}{figure.caption.21}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Comparison of the MSE of the different estimators varying the difference of the means.\relax }}{18}{figure.caption.21}
\contentsline {figure}{\numberline {3.5}{\ignorespaces MSE for each setting. Results are averaged over 2000 experiments.\relax }}{22}{figure.caption.25}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Increasing number of impressions.}}}{22}{figure.caption.25}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Increasing number of ads.}}}{22}{figure.caption.25}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Increasing value of maximum CTR.}}}{22}{figure.caption.25}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Relative player 1 utility gain for different value of the bid defined as $\frac {utility(b)}{utility(v)} - 1$. Results are averaged over 2000 experiments.\relax }}{23}{figure.caption.27}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Grid world results with the three reward functions averaged over 10000 experiments. Optimal policy is the black line.\relax }}{24}{figure.caption.29}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Bernoulli.}}}{24}{figure.caption.29}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\mathcal {N}(-1, 5)$.}}}{24}{figure.caption.29}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\mathcal {N}(-1, 1)$.}}}{24}{figure.caption.29}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Profit per year averaged over 100 experiments.\relax }}{25}{figure.caption.30}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Training phase.}}}{25}{figure.caption.30}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Test phase.}}}{25}{figure.caption.30}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Mean bias obtained by ME, DE and $\text {WE}_{\infty }$ with different sample sizes and bins (only for ME and DE). \relax }}{27}{figure.caption.33}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Variance of the bias obtained by ME, DE and $\text {WE}_{\infty }$ with different sample sizes and bins. \relax }}{28}{figure.caption.34}
\contentsline {figure}{\numberline {3.11}{\ignorespaces Average reward averaged on 100 experiments.\relax }}{29}{figure.caption.36}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Discrete actions.}}}{29}{figure.caption.36}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Continuous actions.}}}{29}{figure.caption.36}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of all the other algorithms and of the best setting of RQ-Learning for this experiment. Results are averaged over $10000$ experiments.\relax }}{38}{figure.caption.37}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{38}{figure.caption.37}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{38}{figure.caption.37}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of the best setting of RQ-Learning for this experiment together with other less effective setting of RQ-Learning. Results are averaged over $10000$ experiments.\relax }}{39}{figure.caption.38}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{39}{figure.caption.38}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.8}}$}}}{39}{figure.caption.38}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the initial state (bottom) in the Noisy Grid World problem of RQ-Learning (Figure \ref {F:hasselt_qdectol}) and windowed RQ-Learning (Figure \ref {F:hasselt_qdecwintol}) with different values of $\eta $ and $k = 0.8$. Results are averaged over $10000$ experiments.\relax }}{40}{figure.caption.39}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {RQ-Learning}}}{40}{figure.caption.39}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Windowed RQ-Learning}}}{40}{figure.caption.39}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Maximum action-value estimate in the Double Chain problem in state $1$ (\ref {F:double_chain_1_1}, \ref {F:double_chain_1_51}) and state $5$ (\ref {F:double_chain_5_1}, \ref {F:double_chain_5_51}). Results are averaged over $500$ experiments.\relax }}{41}{figure.caption.40}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{41}{figure.caption.40}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{41}{figure.caption.40}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{41}{figure.caption.40}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{41}{figure.caption.40}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Learning rate of the two actions in the Double Chain problem in state $1$ (\ref {F:lrs_1_1}, \ref {F:lrs_1_51}) and state $5$ (\ref {F:lrs_5_1}, \ref {F:lrs_5_51}) for RQ-Learning with and without windowed variance estimation. Results are averaged over $500$ experiments.\relax }}{42}{figure.caption.41}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{42}{figure.caption.41}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{42}{figure.caption.41}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{42}{figure.caption.41}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{42}{figure.caption.41}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Action with maximum value in the Double Chain problem in state $1$ and state $9$ for Q-Learning and windowed RQ-Learning.\relax }}{43}{figure.caption.42}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)}$}}}{43}{figure.caption.42}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\alpha = \genfrac {}{}{}0{1}{n(s,a)^{0.51}}$}}}{43}{figure.caption.42}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Structure of the double-chain problem.\relax }}{43}{figure.caption.43}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the Grid World with Holes problem in the initial state (bottom) of all the other algorithms and of the best setting of RQ-Learning for this experiment. Results are averaged over $10000$ experiments.\relax }}{44}{figure.caption.44}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Structure of the Grid World with Holes problem.\relax }}{44}{figure.caption.45}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Mean reward per step (top) and maximum action-value estimate in the Noisy Grid World problem in the initial state (bottom) of SARSA and of the on-policy windowed version of RQ-Learning for this experiment. Results are averaged over $1000$ experiments.\relax }}{45}{figure.caption.46}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Results on the \textbf {Taxi} problem. Figure~\ref {F:taxi_online} shows the performance of the online variance estimation in terms of mean reward per step. The plots on the left show results using the setting of the policies parameters that is optimal in evaluation; the ones on the right consider parameters optimal in training. An epoch corresponds to $1000$ training steps. Figure~\ref {F:taxi_grid} shows the structure of the grid where S is the initial position of the agent, P is a passenger and G is the goal. Figure~\ref {F:taxi_boot} shows the performance during training of the bootstrapping approach in terms of mean reward per step.\relax }}{55}{figure.caption.49}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Online variance}}}{55}{figure.caption.49}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Taxi grid}}}{55}{figure.caption.49}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Bootstrapped variance}}}{55}{figure.caption.49}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Figure~\ref {F:mountain_car} shows the mean cumulative reward in train and evaluation for Mountain Car, while Figure~\ref {F:acrobot} shows the mean cumulative reward in evaluation for Acrobot. Evaluation epochs are performed every $20$ training episodes for the former, and every $1000$ training steps for the latter.\relax }}{56}{figure.caption.50}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Mountain Car}}}{56}{figure.caption.50}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Acrobot}}}{56}{figure.caption.50}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Mean cumulative reward in evaluation. An epoch is performed every $250000$ steps.\relax }}{56}{figure.caption.51}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Pong}}}{56}{figure.caption.51}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Breakout}}}{56}{figure.caption.51}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Optimal parameter settings search during evaluation in Taxi.\relax }}{57}{figure.caption.52}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Optimal parameter settings search during training in Taxi.\relax }}{57}{figure.caption.53}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Optimal parameter settings search during training in Mountain Car.\relax }}{57}{figure.caption.54}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Results of TS on Taxi with different upper bounds on variance.\relax }}{57}{figure.caption.55}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Results of TS on Taxi using Hoeffding upper bound with different values of the $c$ parameter.\relax }}{58}{figure.caption.56}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Results of TS on Taxi using Hoeffding upper bound with $c=2$ without and with $\chi ^2$ upper bound boost.\relax }}{58}{figure.caption.57}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Average reward averaged on 10 experiments. \relax }}{61}{figure.caption.60}
\addvspace {10\p@ }
\addvspace {10\p@ }
