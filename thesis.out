\BOOKMARK [1][]{chapter*.3}{List of Figures}{}% 1
\BOOKMARK [1][]{chapter*.4}{List of Algorithms}{}% 2
\BOOKMARK [0][]{section*.6}{Glossary}{}% 3
\BOOKMARK [-1][]{part.1}{I Starting Point}{}% 4
\BOOKMARK [0][]{chapter.1}{Introduction}{part.1}% 5
\BOOKMARK [1][]{section.1.1}{Perception and interaction}{chapter.1}% 6
\BOOKMARK [1][]{section.1.2}{Learn how to act with Reinforcement Learning}{chapter.1}% 7
\BOOKMARK [2][]{subsection.1.2.1}{Uncertainty in Reinforcement Learning}{section.1.2}% 8
\BOOKMARK [2][]{subsection.1.2.2}{Balancing exploration and exploitation}{section.1.2}% 9
\BOOKMARK [1][]{section.1.3}{My research}{chapter.1}% 10
\BOOKMARK [2][]{subsection.1.3.1}{What is my research about}{section.1.3}% 11
\BOOKMARK [2][]{subsection.1.3.2}{What we have done}{section.1.3}% 12
\BOOKMARK [2][]{subsection.1.3.3}{Outline of the thesis}{section.1.3}% 13
\BOOKMARK [0][]{chapter.2}{Preliminaries}{part.1}% 14
\BOOKMARK [1][]{section.2.1}{Agent and environment}{chapter.2}% 15
\BOOKMARK [1][]{section.2.2}{Markov Decision Processes}{chapter.2}% 16
\BOOKMARK [2][]{subsection.2.2.1}{Value functions}{section.2.2}% 17
\BOOKMARK [1][]{section.2.3}{Solving an MDP}{chapter.2}% 18
\BOOKMARK [2][]{subsection.2.3.1}{Dynamic Programming}{section.2.3}% 19
\BOOKMARK [2][]{subsection.2.3.2}{Reinforcement Learning}{section.2.3}% 20
\BOOKMARK [-1][]{part.2}{II Bellman Update}{}% 21
\BOOKMARK [0][]{chapter.3}{Maximum Expected Value Estimation}{part.2}% 22
\BOOKMARK [1][]{section.3.1}{Problem definition}{chapter.3}% 23
\BOOKMARK [2][]{subsection.3.1.1}{Related works}{section.3.1}% 24
\BOOKMARK [1][]{section.3.2}{Weighted Estimator}{chapter.3}% 25
\BOOKMARK [2][]{subsection.3.2.1}{Generalization to infinite random variables}{section.3.2}% 26
\BOOKMARK [1][]{section.3.3}{Analysis of Weighted Estimator}{chapter.3}% 27
\BOOKMARK [2][]{subsection.3.3.1}{Bias}{section.3.3}% 28
\BOOKMARK [2][]{subsection.3.3.2}{Variance}{section.3.3}% 29
\BOOKMARK [1][]{section.3.4}{Maximum Expected Value estimation in Reinforcement Learning}{chapter.3}% 30
\BOOKMARK [2][]{subsection.3.4.1}{Online}{section.3.4}% 31
\BOOKMARK [2][]{subsection.3.4.2}{Batch}{section.3.4}% 32
\BOOKMARK [2][]{subsection.3.4.3}{Deep Reinforcement Learning}{section.3.4}% 33
\BOOKMARK [1][]{section.3.5}{Empirical results}{chapter.3}% 34
\BOOKMARK [2][]{subsection.3.5.1}{Discrete states and action spaces}{section.3.5}% 35
\BOOKMARK [2][]{subsection.3.5.2}{Continuous state spaces}{section.3.5}% 36
\BOOKMARK [0][]{chapter.4}{Exploiting uncertainty of the Bellman operator components to deal with highly stochastic problems}{part.2}% 37
\BOOKMARK [1][]{section.4.1}{Preliminaries}{chapter.4}% 38
\BOOKMARK [1][]{section.4.2}{The proposed method}{chapter.4}% 39
\BOOKMARK [2][]{subsection.4.2.1}{Decomposition of the TD error}{section.4.2}% 40
\BOOKMARK [2][]{subsection.4.2.2}{Analysis of the decomposed update}{section.4.2}% 41
\BOOKMARK [2][]{subsection.4.2.3}{Variance dependent learning rate}{section.4.2}% 42
\BOOKMARK [2][]{subsection.4.2.4}{Discussion on convergence}{section.4.2}% 43
\BOOKMARK [1][]{section.4.3}{Experimental results}{chapter.4}% 44
\BOOKMARK [2][]{subsection.4.3.1}{Noisy Grid World}{section.4.3}% 45
\BOOKMARK [2][]{subsection.4.3.2}{Double Chain}{section.4.3}% 46
\BOOKMARK [2][]{subsection.4.3.3}{Grid World with Holes}{section.4.3}% 47
\BOOKMARK [2][]{subsection.4.3.4}{On-policy learning}{section.4.3}% 48
\BOOKMARK [-1][]{part.3}{III Uncertainty-Driven Exploration}{}% 49
\BOOKMARK [0][]{chapter.5}{Thompson Sampling Based Algorithms for Exploration in Reinforcement Learning}{part.3}% 50
\BOOKMARK [1][]{section.5.1}{Related work}{chapter.5}% 51
\BOOKMARK [1][]{section.5.2}{Thompson Sampling in value-based Reinforcement Learning}{chapter.5}% 52
\BOOKMARK [1][]{section.5.3}{Efficient uncertainty estimation}{chapter.5}% 53
\BOOKMARK [2][]{subsection.5.3.1}{Online estimation}{section.5.3}% 54
\BOOKMARK [2][]{subsection.5.3.2}{Bootstrapping}{section.5.3}% 55
\BOOKMARK [1][]{section.5.4}{Experiments}{chapter.5}% 56
\BOOKMARK [2][]{subsection.5.4.1}{Discrete state space}{section.5.4}% 57
\BOOKMARK [2][]{subsection.5.4.2}{Continuous state space}{section.5.4}% 58
\BOOKMARK [2][]{subsection.5.4.3}{Deep Reinforcement Learning}{section.5.4}% 59
\BOOKMARK [0][]{chapter.6}{Exploration Driven by an Optimistic Bellman Equation}{part.3}% 60
\BOOKMARK [1][]{section.6.1}{Learning value function ensembles with optimistic estimate selection}{chapter.6}% 61
\BOOKMARK [2][]{subsection.6.1.1}{An optimistic Bellman Equation for action-value function ensembles}{section.6.1}% 62
\BOOKMARK [2][]{subsection.6.1.2}{Relation to Intrinsic Motivation}{section.6.1}% 63
\BOOKMARK [1][]{section.6.2}{Optimistic value function estimators}{chapter.6}% 64
\BOOKMARK [2][]{subsection.6.2.1}{Optimistic Q-Learning}{section.6.2}% 65
\BOOKMARK [2][]{subsection.6.2.2}{Optimistic Deep Q-Network}{section.6.2}% 66
\BOOKMARK [1][]{section.6.3}{Experimental evaluation}{chapter.6}% 67
\BOOKMARK [2][]{subsection.6.3.1}{Settings}{section.6.3}% 68
\BOOKMARK [2][]{subsection.6.3.2}{Results}{section.6.3}% 69
\BOOKMARK [-1][]{part.4}{IV Final Remarks}{}% 70
\BOOKMARK [0][]{chapter.7}{Conclusion}{part.4}% 71
\BOOKMARK [1][]{section.7.1}{Recap of the thesis}{chapter.7}% 72
\BOOKMARK [2][]{subsection.7.1.1}{Bellman update}{section.7.1}% 73
\BOOKMARK [2][]{subsection.7.1.2}{Exploration}{section.7.1}% 74
\BOOKMARK [2][]{subsection.7.1.3}{Comments}{section.7.1}% 75
\BOOKMARK [1][]{section.7.2}{Future directions}{chapter.7}% 76
\BOOKMARK [0][]{section*.87}{Bibliography}{part.4}% 77
\BOOKMARK [0][]{appendix.A}{Mushroom}{part.4}% 78
\BOOKMARK [1][]{section.A.1}{Related works}{appendix.A}% 79
\BOOKMARK [1][]{section.A.2}{Ideas and Concepts}{appendix.A}% 80
\BOOKMARK [1][]{section.A.3}{Design}{appendix.A}% 81
