\chapter{Conclusion}
The end of three years of Ph.D research is surely a significant moment. I started my Ph.D. without being sure that research was made for me and whether I would have been able to do a good Ph.D or not. Happily, after all this experience I can be very satisfied of what I have achieved and learned.
This thesis is the result of three years where I studied and worked on amazing topics that are one of the most important of the current decade and, presumably, of the next ones. In particular, having the feeling of being part in the progress of such a revolutionary scientific topic, represented for me a very significant achievement. Moreover, considering that I had the possibility to participate to top conferences around the world, to work in an excellent team and to be followed by a great advisor which I thank for his help, I can say this Ph.D. gave me what I was hoping for and maybe more.

\section{Recap of the thesis}
This thesis resumes the work I made during my three years of Ph.D. research about the exploitation of uncertainty in Reinforcement Learning (RL). After a first part with the introduction about general consideration on \gls{rl} and the description of preliminary concepts, the thesis is split in two parts where it is showed how uncertainty can be exploited to improve performance of state-of-the-art algorithms. In the former part, uncertainty is used to improve the estimate of the components of the update by means of the Bellman operator; in the latter part, uncertainty is used to drive exploration aiming to improve sample-efficiency of exploration policies.

The exploitation of uncertainty is not a new line of research in \gls{rl}, thus my works are mainly inspired by methodologies available in literature that I considered to present my own novel ones. 

\section{Future directions}
