\chapter{Conclusion}
The end of three years of Ph.D research is surely a significant moment. I started my Ph.D. without being sure of what the world of research was like and whether I would have been able to do a good Ph.D or not. Happily, after all this experience I can be very satisfied of what I have achieved and learned.
This thesis is the result of three years in which I studied and worked on amazing topics that are one of the most important of the current decade and, presumably, of the next ones. In particular, having the feeling of being part of the huge community of researchers aiming to the progress of such a revolutionary scientific topic, represented a very significant achievement for me. Moreover, considering that I had the possibility to participate in the most important conferences around the world, to work in an excellent team and to be followed by a great advisor who I thank for his help, I can say this Ph.D. gave me what I hoped and maybe more.

\section{Recap of the thesis}
This thesis describes the results we obtained during my three years of Ph.D. research about the exploitation of uncertainty in Reinforcement Learning (RL). After a first part with the introduction about general considerations on \gls{rl} and the description of some preliminary concepts, the thesis is split into two parts where it is showed how the uncertainty on action-value estimates can be exploited to improve the performance of state-of-the-art algorithms. In the former part, uncertainty is used to improve the estimate of the components of the equation of the action values by means of the Bellman operator; in the latter part, uncertainty is used to drive exploration in order to improve sample-efficiency of exploration policies. The exploitation of uncertainty is not a new line of research in \gls{rl}, thus our works are mainly inspired by the methodologies available in the literature that we considered to work out our own novel ones.

\subsection{Bellman update}
\subsubsection{What we studied}
The first work we dealt with, after the initial study of the \gls{rl} literature and the state-of-the-art works, was the Double $Q$-Learning (DQL)~\cite{van2010double}. This paper addresses the problem of overestimating the Maximum Expected Value (MEV) in the context of action-value function estimation. This happens, for instance, in $Q$-Learning (QL)~\cite{smith2006optimizer} because of the Maximum Estimator (ME) involved in the update rule via the Bellman Equation (BE). The importance of \gls{dql} stands on the proposal of a variant of \gls{ql} that replaces the \gls{me} with the Double Estimator (DE) making \gls{dql} able to avoid overestimation providing, on the contrary, an underestimation of the optimal action values. The underestimation helps to have a good learning in some problems with high stochasticity and, more generally, in problems where there is not clearly an action that is better than the others. One of the interesting aspects of the \gls{de} is that it can be used in several value-based \gls{rl} algorithms without major changes. For instance, the offline value-based algorithm of Fitted $Q$-Iteration (FQI)~\cite{ernst2005tree} can be easily modified to make it use \gls{de}, resulting in what we call Double Fitted $Q$-Iteration (DDFQI) algorithm. Moreover, the ideas behind \gls{dql} have been also applied in Deep RL (DRL) with the Double Deep $Q$-Network (DDQN)~\cite{hasselt2015double}, a variant of Deep $Q$-Network (DQN)~\cite{mnih2015human} that we also considered during my research on this topic.

\subsubsection{What we did}
To address the problem of overestimating the \gls{mev}, we worked on introducing a new estimator called Weighted Estimator (WE) that computes an weighted average of action-values where the weights are the probability of each action to be the best one. We showed how the \gls{we} can be both positively or negatively biased, but its absolute bias is always lower than the ones of \gls{me} and \gls{de}. We applied this estimator to \gls{ql} leading to Weighted $Q$-Learning (WQL) which we compare to \gls{ql} and \gls{dql} in several discrete problems. Then, we proposed an extension of \gls{we} to continuous problems by means of a variant of \gls{fqi}, called Weighted Fitted $Q$-Iteration (WFQI), which uses \gls{we} and Gaussian Process (GP) regression to estimate the uncertainty of the prediction. The interesting aspect of this method is that it adapts naturally to problems with continuous actions, being one of the few value-based approaches that can handle infinite action spaces. Eventually, we studied the application of \gls{we} also in \gls{drl} by introducing a variant of \gls{dqn} called Weighted Deep $Q$-Network. The adaptation of \gls{we} in \gls{dqn} is not straightforward since the estimation of the uncertainty is cumbersome to obtain in neural networks. To make it practical, we took inspiration from the network architecture proposed in Bootstrapped Deep $Q$-Network (BDQN)~\cite{osband2017deep} which uses a single network that is split into multiple outputs resulting in an ensemble of action-value function estimates. Then, we used the estimates provided by the ensemble to work out the uncertainty to use in the computation of \gls{we}. This method is a first practical solution to use \gls{we} in \gls{dqn}, but the unsatisfactory preliminary results and the possibility of using different methods to compute the uncertainty make this work only a first solution in this direction.

The study of the overestimation of \gls{mev} is not the only way we considered improving the updating of action-value function estimate via the \gls{be}. Our second work about this introduces the $RQ$-Learning algorithm which is the result of the effort put into decomposing the \gls{be} and exploiting the uncertainty of its components, i.e. the reward and the maximum action-value. This is done considering the fact that the sources of uncertainty of the two components of the \gls{be} are different and, thus, make sense to consider them separately. Together with this consideration, a different learning rate is used for the two components and adapted according to the measure of uncertainty. The method is compared with several variants of \gls{ql} (e.g. \gls{dql} and \gls{wql}) showing significant results and robustness in heterogeneous problems. We also proposed an on-policy variant of the methodology and compared it with SARSA achieving good results in this case too.

\subsection{Exploration}
\subsubsection{What we studied}
The problem of exploration in \gls{rl} is one of the most addressed in the \gls{rl} literature. It deals with the agent's purpose to cover the a significant portion of the state space of the environment in order to improve the quality of the learned policy. However, the balance between exploration and exploitation is critical in order to obtain higher performance in terms of the cumulative discounted reward; therefore, an exploration policy that minimizes the number of samples needed to learn an effective exploitative policy is desirable. Among the trivial $\varepsilon$-greedy strategy that makes no use of the information acquired by the agent, the Boltzmann and mellowmax policy~\cite{asadi2016alternative} use the current estimate of the action-value function, computing the next action to be executed by a soft-max operator.

More complex works to deal with exploration are based on the concept of Optimism in the Face of Uncertainty (OFU) which encourages the execution of actions that lead to unknown regions of the state space. This is pursued, among others, by strategies based on Thompson Sampling (TS)~\cite{thompson1933likelihood} and by the algorithms belonging to the Intrinsic Motivation (IM)~\cite{schmidhuber1991possibility} category. In exploration strategies based on TS, e.g. $Q$-value sampling~\cite{dearden1998bayesian}, the actions are sampled from the distribution modeling their probability of being the ones with the highest action value. This has been widely studied in the context of Multi-Armed Bandit problems, but it has been proven to be an effective way to balance exploration and exploitation also in the context of \gls{rl}~\cite{auer2007logarithmic}. On the other hand, the \gls{im} strategies add an intrinsic reward, which is summed to the reward returned by the environment, expressing the quality of the reached state in terms of novelty. In particular, the intrinsic reward is usually proportional to the amount of new knowledge of the state space obtained by the agent when reaching it; in this way, the agent is intuitively encouraged to visit new kinds of states. The challenging part of \gls{im} is to work out a measure to evaluate the novelty of a state, which could significantly slow down learning. Among others, some works address this issue by proposing different ways of counting the number of similar states reached~\cite{tang2017exploration} and others introduce new measure such as the agent's curiosity~\cite{schmidhuber1991possibility, pathak2017curiosity}.

\subsubsection{What we did}
We addressed the exploration problem by analyzing both the \gls{ts} and \gls{im} strategies that are both based on \gls{ofu}. In the first work, we introduced some algorithms to make the use of exploration strategies based on \gls{ts} practical in \gls{rl}. Indeed, several methodologies based on \gls{ts} have certainly desirable theoretical properties, but perform poorly in empirical applications. The main contribution of this work is the proposal of different ways of computing uncertainty and pursuing the \gls{ofu} principle. In particular, the first approach is developed for discrete problems and exploits the computation of uncertainty explained in the work about \gls{we}~\cite{deramo2016estimating} also applying statistical upper bounds to encourage exploration, while the second way is proposed as a solution for continuous high dimensional problems where the uncertainty is estimated via an ensemble of action-value function approximator as proposed in \gls{bdqn}. The different algorithms are empirically evaluated in increasingly complex problems in which exploration is a critical aspect to solve them. The results highlight how they are able to achieve better performance faster than other sampling strategies, thus showing a better balancing between exploration and exploitation.

The second work in this direction is more based on \gls{im} and proposes a variant of the \gls{be} called Optimistic \gls{be} (OBE). In \gls{obe}, an ensemble of action-value estimates is used to compute the update of the action values based on a maximum-entropy principle. The entropy maximization aims to increase the action values proportionally to their uncertainty provided by the ensemble in such a way to encourage exploration. This work is inspired by the \gls{im} literature because of the intrinsic exploration bonus involved in the update of the action-value, but with the desirable property of not using any time-consuming way to explicitly measure uncertainty. The \gls{obe} is applied in variants of \gls{ql} and \gls{dqn}, which we called Optimistic \gls{ql} (OQL) and Optimistic \gls{dqn} (ODQN) respectively. These methodologies are theoretically studied, e.g. the convergence of \gls{oql} is proven, and empirically evaluated against Bootstrapped Q-Learning (BQL) and \gls{bdqn}, showing better performance in the considered problems.

\subsection{Comments}
All the described works have been done in collaboration with colleagues and/or master students working on their MSc thesis, together with the help of my advisor. While I think the works about the improvement in the Bellman update \gls{be} have been studied enough in depth, I think there is still work to be done on exploration. In more detail, the study of exploration strategies based on \gls{ts} brought to satisfactory results, but the development of theoretical guarantees is lacking to make this work interesting for publication. On the other hand, the work about the \gls{obe} is currently under review, but I think it needs to be better studied especially in its empirical evaluation.

As I focused on the problems described before, I have always considered the progress in the Deep Reinforcement Learning (DRL) literature. During my Ph.D., this field has become more and more important to the point that the empirical evaluation of the proposed novel works had to be done on \gls{drl} problems, e.g. Atari games~\cite{bellemare13arcade}, to make them more appealing. In three years, I tried to work on \gls{drl} studying novel ways to improve state-of-the-art methods, e.g. smart feature extraction in \gls{dqn}, and these works brought to the publication of some MSc thesis, but were not significant enough to be published at conferences. The major problems I faced working on \gls{drl} were the very high computational demand in terms of resources and time to the point that I had to wait several days to obtain the results for a single experiment. Considering the slowness of working on \gls{drl}, I preferred to focus on practical classical \gls{rl} problems and secondly extend these works in \gls{drl} applications, e.g. applying \gls{we} in \gls{dqn} by introducing Weighted \gls{dqn} (WDQN).

\section{Future directions}
All the works done for this thesis, the others I did but which are out of the focus, and all the ones I studied in the literature, gave me a lot of insights on the research in \gls{rl} and on the important problems that need to be addressed to improve the state of the art of this field. In particular, the current effort in solving highly-dimensional complex problems is resulting in outstanding performance, as in the case of chess~\cite{silver2017chess}, but is sometimes ignoring the huge increase in computational resources to solve these problems. Indeed, typically a \gls{drl} model has an enormous amount of parameters, requires days of training time corresponding to many years of human play, often has unstable learning and most often the semantics of the learned representation is hard to understand to the point that it is often used in a black-box way~\cite{mnih2015human}. As a matter of fact, despite the excellent performance that a \gls{drl} model and more in general a \gls{dl} model is able to achieve, this last drawback causes serious problems of robustness of the learned representations leading to unsatisfactory results. For instance, in a recent work on image classification with deep neural networks, the authors show how it is possible to trick the model by means of samples specifically modified for this purpose~\cite{yuan2017adversarial}. These ``adversarial examples'' are usually samples of slightly modified images in such a way that, for instance, a single pixel is changed. The pixel is not changed randomly, on the contrary its value is smartly modified by studying the gradient of the function learned by the classifier. In the end, the original sample and the adversarial one look totally the same at a human eye, but the classifier sees them as completely different samples. The reasons behind this issue are multiple and the analysis of them requires a much longer discussion that is out of focus for this last section of the thesis, but substantially it can be concluded that this issue demonstrates the lack of robustness of the \gls{dl} model and, more in general, the difficulty for the model to extract the real semantics of what it learns.

\begin{figure}[t]
\begin{minipage}{\textwidth}
\begin{center}
  \subfigure[Breakout\label{F:breakout_frame}]{\includegraphics[scale=.065]{img/breakout.jpg}}
  \hspace{1cm}
  \subfigure[Humanoid\label{F:humanoid}]{\includegraphics[scale=.3]{img/humanoid.jpg}}
\end{center}
\end{minipage}
\caption[Breakout and Humanoid problems]{Graphical rendering of Breakout and Humanoid problems.}\label{F:breakout_humanoid}
\end{figure}
Considering that the models and their fitting algorithms used in \gls{drl} are the same as those of \gls{dl}, it is intuitive how they can suffer from the previously described issues with an impact on the quality of the learned policy. Figure~\ref{F:breakout_humanoid} shows two examples of environments to highlight problems that may arise when learning how to solve them. In particular, Figure~\ref{F:breakout_frame} refers to the Breakout game briefly described in Section~\ref{S:exploration_drl}. Recalling that in this game the purpose is to catch the ball with the platform on the bottom so as to make it bounce and hit the bricks on the wall at the top, it is intuitive how the number of features needed to solve this games is much less than the whole number of pixels of the raw frame. Thus, hopefully the deep $Q$-network should be able to extract only the few relevant features, such as the coordinates of the ball and the platform, in order to simplify the policy learning. However, this usually does not happen and the network learns a very abstract representation of the input whose semantics is practically not interpretable. On the other hand, Figure~\ref{F:humanoid} refers to a control problem in which a humanoid walker is taught how to walk avoiding obstacles. In this case, the problem lies in the quality of the learned policy to make the humanoid walk. Indeed, usually the policy learned by the agent is effective for solving the environment, but let the humanoid walk in a very unnatural way, very different from the human way of walking.

These examples show that the outcome of the learning process of a \gls{dl} model is almost always very unpredictable and it is unlikely to resemble the model learned by a human being. This happens because is very natural for a human to extract the semantics behind what he/she sees, while for a \gls{dl} model this is generally not a natural way of learning. Taking these considerations into account, I believe that helping \gls{drl} models to extract the real semantics of a problem is a promising way of trying to improve the performance of \gls{drl} algorithms. To this end, the field of Multi-Task learning aims to train a single model on multiple problems. In this way, the knowledge about each task is shared with the one about the other tasks and this helps to force the model in extracting features with significant semantics, just like a regularization method. To this end, I think \gls{drl} is particularly suited thanks to the huge amount of training parameters that may intuitively allow the model to be able to learn multiple problems. Indeed, recently proposed works in \gls{drl} about Multi-Task~\cite{higgins2017darla, ammar2014online, teh2017distral} have shown promising results and I believe that more effort should be put in this direction to make \gls{drl} algorithms more and more efficient.