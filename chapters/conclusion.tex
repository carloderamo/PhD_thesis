\chapter{Conclusion}
The end of three years of Ph.D research is surely a significant moment. I started my Ph.D. without being sure of what the world of research is like and whether I would have been able to do a good Ph.D or not. Happily, after all this experience I can be very satisfied of what I have achieved and learned.
This thesis is the result of three years where I studied and worked on amazing topics that are one of the most important of the current decade and, presumably, of the next ones. In particular, having the feeling of being part in the progress of such a revolutionary scientific topic, represented for me a very significant achievement. Moreover, considering that I had the possibility to participate to top conferences around the world, to work in an excellent team and to be followed by a great advisor which I thank for his help, I can say this Ph.D. gave me what I was hoping for and maybe more.

\section{Recap of the thesis}
This thesis resumes the work I made during my three years of Ph.D. research about the exploitation of uncertainty in Reinforcement Learning (RL). After a first part with the introduction about general consideration on \gls{rl} and the description of preliminary concepts, the thesis is split in two parts where it is showed how uncertainty can be exploited to improve performance of state-of-the-art algorithms. In the former part, uncertainty is used to improve the estimate of the components of the update by means of the Bellman operator; in the latter part, uncertainty is used to drive exploration aiming to improve sample-efficiency of exploration policies. The exploitation of uncertainty is not a new line of research in \gls{rl}, thus my works are mainly inspired by methodologies available in literature that I considered to work out my own novel ones

\subsection{Bellman update}
\subsubsection{What I studied}
The first work I dealt with, after the initial study of the classical literature and the state-of-the-art works, was the Double Q-Learning (DQL)~\cite{van2010double}. This paper addresses the problem of overestimation of the Maximum Expected Value (MEV) in the context of action-value function estimation. This happens, for instance, in Q-Learning (QL)~\cite{smith2006optimizer} because of the Maximum Estimator (ME) involved in the update rule via Bellman Equation (BE). The importance of \gls{dql} stands on the proposal of a variant of \gls{ql} which replaces the \gls{me} with the Double Estimator (DE) making \gls{dql} able to avoid the overestimation providing, on the contrary, an underestimation of the optimal action-values. The underestimation helps to have good learning in some problems with high stochasticity and, more in general, in problems where there is not clearly an action which is better than the others. One of the interesting aspects of the \gls{de} is that it can be used in several value-based \gls{rl} algorithms without major changes. For instance, the offline value-based algorithm of Fitted $Q$-Iteration (FQI)~\cite{ernst2005tree} can be easily modified to make it use \gls{de} resulting in what we call Double Fitted $Q$-Iteration algorithm. Moreover, the ideas behind \gls{dql} have been also applied in Deep RL (DRL) with the Double Deep $Q$-Network (DDQN)~\cite{hasselt2015double}, a variant of Deep $Q$-Network~\cite{mnih2015human} which I also considered during my research on this topic.

\subsubsection{What I did}
To address the problem of overestimation of \gls{mev}, I worked on the introduction of a novel estimator called Weighted Estimator (WE) which computes an averaged sum of action-values where the weights are the probability of each action to be the best one. I showed how the \gls{we} can be both positively or negatively biased, but the its absolute bias is always less than the ones of \gls{me} and \gls{de}. I applied this estimator to \gls{ql} bringing to Weighted $Q$-Learning (WQL) which I compare to \gls{ql} and \gls{dql} in several discrete problems. Then, I proposed an extension of \gls{we} to continuous problems by means of a variant of \gls{fqi} which uses \gls{we} called Weighted Fitted $Q$-Iteration (WFQI) and Gaussian Process (GP) regression to estimate the uncertainty of the prediction. The interesting aspect of this method is that it naturally adapts to problems with continuous actions being one of the few value-based approaches able to deal with infinite action spaces. Eventually, I studied the application of \gls{we} also in \gls{drl} introducing a variant of \gls{dqn} called Weighted Deep $Q$-Network. The adaptation of \gls{we} in \gls{dqn} is not straightforward since the computation of the uncertainty is cumbersome to obtain in neural networks. To make it practical, I took inspiration from the network architecture proposed in Bootstrapped Deep $Q$-Network (BDQN)~\cite{osband2017deep} which uses a single network which is split in multiple output resulting in an ensemble of action-value function estimates. Then, I used the estimates provided by the ensemble to work out the uncertainty to use in the computation of \gls{we}. This method is a first practical solution to use \gls{we} in \gls{dqn}, but the not satisfying preliminary results and the possibility to use different methods to compute uncertainty make this work only a first solution in this direction.

The study of the overestimation of \gls{mev} is not the only way I considered to improve the update of action-value function estimate via the \gls{be}. My second work about this introduces the $RQ$-Learning algorithm which is the result of the effort put in decomposing the \gls{be} and exploiting the uncertainty of its components, i.e. the reward and the maximum action-value. This is done considering the fact that the source of uncertainty of the two components of the \gls{be} are different and, thus, make sense to consider them separately. Together with this consideration, a different learning rate for the two components is used and adapted according to the measure of uncertainty. The method is compared with several variant of \gls{ql} (e.g. \gls{dql} and \gls{wql}) showing significant results and robustness in heterogeneous problems. I also proposed an on-policy variant of the methodology and compared it with SARSA reaching good results also in this case.

\subsection{Exploration}

\subsubsection{What I studied}

\subsubsection{What I did}

\subsubsection{Deep Reinforcement Learning}

\subsection{Comments}

\section{Future directions}
