\chapter*{Abstract}
\lettrine{T}{he} recent exponential growth of the research about Reinforcement Learning (RL) has been made possible by the equally significant improvement in the computational power. Indeed the coming of powerful and relatively affordable hardware, in particular Graphic Processing Units (GPUs), allowed researchers to extend the study of RL methodologies to highly-dimensional problems that were unpractical before opening the line of research to that field which is now commonly known under the name of Deep Reinforcement Learning (DRL). However, the groundbreaking results that DRL is being able to achieve are being obtained at the cost of a huge amount of sample needed to learn together with very large learning time, usually in the order of days. One of the reasons why this is happening, besides the outstanding significance of the obtained results which is basically putting the problems of efficiency of these methodologies in the background, relies on the fact that often the experiments are run in simulation where the sample-efficiency problem is not such an issue as in real applications. Nevertheless an effort to improve the sample-efficiency and other issues of many DRL algorithms, e.g. stability of learning, is being made by several recent works that proved to be able to address this problem successfully.

The purpose of this thesis is to address the previously described problems, that are classical issues of RL and not only of DRL, proposing novel methodologies that explicitly consider the concept of \textit{uncertainty} to speedup learning and improve its stability. 
