\chapter*{Abstract}
\lettrine{T}{he} recent exponential growth of the research about Reinforcement Learning (RL) has been made possible by the equally significant improvement in the computational power. Indeed the coming of powerful and relatively affordable hardware, in particular Graphic Processing Units (GPUs), allowed researchers to extend the study of RL methodologies to highly-dimensional problems that were unpractical before, opening the line of research which is now commonly known under the name of Deep Reinforcement Learning (DRL). However, the groundbreaking results that DRL is achieving are being obtained at the cost of a huge amount of samples needed to learn, together with very large learning time usually in the order of days. One of the reasons why this is happening, besides the outstanding significance of the obtained results which is basically putting the problems of efficiency of these methodologies in the background, relies on the fact that often the experiments are run in simulation where the sample-efficiency problem is not such an issue as in real applications. Nevertheless an effort to improve the sample-efficiency and other issues of many DRL algorithms, e.g. stability of learning, is being made by several recent works that proved to be able to address this problem successfully.

The purpose of this thesis is to address the previously described problems, that are classical issues of RL and not only of DRL, proposing novel methodologies that explicitly consider the concept of \textit{uncertainty} to speedup learning and improve its stability. Indeed, since a relevant goal of a RL agent is to reduce its uncertainty about the environment it is moving in, taking uncertainty explicitly into account can intuitively be an effective way to act. This solution is not new in RL research, but there is still a lot of work that can be done in this direction. This thesis takes inspiration from the available literature about the topic and extends it with novel significant improvement on the state-of-the-art. In particular, the works included in this thesis can be grouped in two parts: one where the uncertainty is used to improve the behavior of the Bellman Equation and the other where it is used to improve exploration. The works belonging to the former group aim to address some of the problems of action-value estimation in the context of value-based RL, in particular in the estimate of the maximum operator involved in the famous $Q$-Learning algorithm and, more in general, in the estimate of the components of the Bellman Equation. On the other hand, the works belonging to the latter group study novel methodology to improve exploration by studying the use of Thompson Sampling in RL or introducing a variant of the Bellman Equation which incorporates an optimistic estimate of the action-value function in order to improve exploration according to the principle of Optimism in the Face of Uncertainty.

All the works presented in this thesis are described, theoretically studied and eventually empirically evaluated on RL problems. The obtained results highlight the benefits that the explicit exploitation of uncertainty in RL algorithms can provide; indeed we show how in a large set of problems that have been chosen with the purpose to highlight particular aspect we were interested in, e.g. exploration capabilities, our methods prove to be more stable and faster to learn than other works available in literature.
