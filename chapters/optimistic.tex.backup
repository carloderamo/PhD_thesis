\chapter{Exploration Driven by an Optimistic Bellman Equation}\label{C:opt}
As described in Chapter~\ref{C:ts}, the \gls{ofu}-based techniques provide an optimistic estimation under uncertainty, encouraging in this way exploration of uncertain region. Optimism can be categorized in
\begin{itemize}
\item \textbf{confidence interval estimation}, such as IEQL+~\cite{meuleau1999exploration} and UCRL~\cite{auer2007logarithmic}, which directly estimates $Q$-value confidence intervals;
 \item \textbf{optimistic initialization} of the action-value function approximator to an optimistic value, a method proposed first by~\cite{sutton1998reinforcement} about which subsequently~\cite{even2002convergence} proved convergence to a near-optimal solution.
\end{itemize}
While the work in Chapter~\ref{C:ts} deals with the first category of optimism-based algorithms, the work described in the following deals with the second one. This is only partially true since we do not consider optimistic initialization, but we want the action-value to be an optimistic estimate which is propagated through the learning by the \gls{be}.
This approach is inspired by the broad category of algorithms based on \gls{im}~\cite{singh2004intrinsically}, that despite having less theoretical guarantees
than Bayesian approaches, have been able to obtain impressive results~\cite{bellemare2016unifying}. As already discussed in Chapter~\ref{C:ts}, \gls{im}
algorithms define an additional \textit{intrinsic} reward, which acts as an exploration bonus. Often, the additional reward is defined using heuristics, such as counting state visits and rewarding less visited states~\cite{ostrovski2017count}, or by \textit{surprise} which is the error in predicting future states~\cite{pathak2017curiosity}. However, the drawback of \gls{im} techniques is their lack of a principled definition of the intrinsic reward for exploration.

Considering this premise, we worked on the proposal of a novel \gls{obe}. The \gls{obe} results in an optimistic $Q$-value estimate
from an ensemble of value functions where the optimistic estimate is obtained from a maximum-entropy principle. For the exploration bonus that \gls{obe} implicitly defines, we can prove that the bonus decreases consistently with the number of
state visits. Our proposed algorithm can be seen as a mixture of
different techniques: as an approximated Bayesian method~\cite{engel2005reinforcement,vlassis2012bayesian} we estimate
the uncertainty with an ensemble~\cite{osband2017deep}; like optimism-based methods~\cite{lai1985asymptotically,kearns2002near,brafman2002r,azizzadenesheli2517efficient} we select
optimistic estimates, and like \gls{im} we propagate an implicit
exploration bonus~\cite{singh2004intrinsically,schmidhuber2008driven,white2010interval}.

\section{Learning value function ensembles with optimistic estimate selection}
\label{sec:obe}
Ensemble methods~\cite{opitz1999popular} constitutes a popular set of \gls{ml} technique where multiple models are used to learn the same target function. In addition to being commonly used to improve the generalization of the prediction, ensemble methods offer a simple way to estimate the uncertainty of the prediction. We consider the application of ensemble methods in the \gls{rl} framework with the purpose of approximating the action-value functions while having an estimate of their uncertainty, in order to apply the \gls{ofu} principle in action selection.

\subsection{An optimistic Bellman equation for action-value function ensembles}
Starting from the purpose of overestimating the action-value functions with the result of encouraging exploration, we propose \gls{obe} which propagates an optimistic estimate of the action-value function. In particular, \gls{obe} is a \gls{be} which incorporates the information about the uncertainty provided by a $Q$-function ensemble. We want to emphasize that, when all the $Q$-functions of the ensemble are identical, we assume that there is no uncertainty and under this condition the \gls{obe} will behave exactly equivalently to the classic \gls{be}. Indeed, we can prove that the fixed point $Q^*$ of \gls{obe} is the same of the classic \gls{be}; in other words, the \gls{obe} differs from the classic \gls{be} when the estimates in the ensemble are different among themselves, which usually happen when the action-value functions are approximated with a limited number of samples or with function approximatore.g. neural networks). This makes sense, since when the perfect solution is available there is no need of optimism and exploration. 
The optimistic Bellman operator derived from the \gls{obe}, enjoys the properties of the classical one, like contractivity and the existence of a unique fixed point, potentially enabling its usage in value-based or actor-critic reinforcement algorithms. 
The diversity in the $Q$-value ensemble should be ideally consistent with the uncertainty of the estimation; e.g. when the estimate is certain, all the values in the ensemble should agree on the same value, otherwise the ensemble should have discordant values.
Given an ensemble of $Q$-value functions $\{Q_m\}_{m=1}^M$, we want to work out an optimistic estimate from the diverse estimates provided by the ensemble. The simplest and most optimistic solution is to select the highest value $\max_m Q_m(s,a)$.
However selecting the highest estimate makes poor use of the information provided by the ensemble and can be sensible to noise. In order to mitigate this effect, we introduce a notion of \textit{belief} over the estimates where $b_m(s,a)$ is the belief of $Q_m(s,a)$. The main idea is to add an entropic regularization term to the objective (i.e., $\max_{b(s,a)} \sum_m b_m(s,a) Q_m(s,a) + c \sum_{m} p_m \log p_m $); or to bound the information loss (i.e., $\sum_m p_m \log p_m < \psi$). Hard constraint on the information loss is more appealing since the introduced hyper-parameter does not depend on the magnitude of the rewards but has no closed-form solution. In contrast, the penalization weighting constant introduced by the soft-constraint regularization term is sensitive to the magnitude of the rewards, but admits a closed-form solution.
We define two different problems where we use an optimistic estimate of the $Q$-value function.

\subsubsection{Entropy-regularized optimistic action-value selection}
We define a \gls{be} over the $Q$-function ensemble by introducing an optimistic estimate penalized by an entropic regularization term.
\begin{probdef}[Regularized version]
\begin{equation}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{rrclcl}
\displaystyle Q_i(s,a) = \max_{b(s,a) \in \mathcal{P}^M} & \multicolumn{3}{l}{R(s,a) + \gamma \sum_m{b}_m(s,a)V_m'(s,a)-\frac{1}{\eta}D_{\mathrm{KL}}\big(p(s,a)\big{\|}u\big) } \\
\textrm{s.t.} & \sum_{m=1}^{M} b_m(s,a) & = & 1 \\
\multicolumn{4}{l}{ \forall s,a,i \in \mathcal{S} \times \mathcal{A} \times \{1,\dots,M\}}
\end{array}\nonumber
\end{equation}\label{PROB:regularized}
\end{probdef}
\noindent with $u_m  =  \nicefrac{1}{M}$, $D_{KL}(b(s,a)\| u)$ is the \gls{kl} divergence between the belief $b(s,a)$ and the uniform distribution $u$, and $V_m'(s,a) \! = \! \sum_{s'} P(s'|s,a)\max_{a'}Q_m(s',a')$.
The choice of using the relative entropy instead of the absolute one has two main advantages: it admits a solution for $\eta \to 0$ and provides a normalization factor.
Since Problem~\ref{PROB:regularized} is a convex constrained problem, it is solvable by dual optimization. Introducing $\lambda$ as Lagrangian multiplier for the constraint, we write the Lagrangian
\begin{eqnarray}
L_i(s,a) \!&=& \!f(s,a; b(s,a)) -\frac{1}{\eta}D_{\mathrm{KL}}\big(b(s,a)\big{\|}u\big)\nonumber \\ && + \lambda\bigg(\sum_m b_m(s,a) - 1\bigg).\label{E:lagrangian}
\end{eqnarray}
Requiring the partial derivatives of $L_i$ w.r.t $p_m$ and $\lambda$ to be zero yields 
\begin{equation}
b_m(s,a) = \frac{e^{\eta  \gamma V_m'(s,a)}}{\sum_{k=1}^{M} e^{\eta \gamma   V_k'(s,a)}}\label{pm}.
\end{equation} 
By substituting $b_m$ in Equation~\ref{E:lagrangian}, we obtain the solution to the problem:
\begin{equation}
Q_i(s,a) = \begin{cases}
\overline{R}(s,a) + \frac{1}{\eta} \log \frac{\sum_{m=1}^Me^{\eta \gamma  V_m'(s,a) }}{M} & \mathrm{if} \eta \neq 0 \label{OBE} \\
\overline{R}(s,a) + \frac{\gamma}M \sum_{m=1}^{M} V_m'(s,a) & \mathrm{otherwise}
\end{cases}.
\end{equation}
Notice that $\eta>0$ leads to a positive (optimistic) biased estimation, while $\eta<0$ will leads to a negative (pessimistic) estimate; in this work we will always assume $\eta>0$ (and therefore we refer to the equation as optimistic). 
However, in general, the choice of $\eta$ is difficult since it depends on the magnitude of the reward function. For this reason we introduce the constrained version of the proposed problem.
 
\subsubsection{Optimistic action-value selection bounding the information loss}
We bound the information loss between the distribution $b_m$ and the uniform distribution to maintain compatibility with Problem~\ref{PROB:regularized}. The information loss is bounded between $-\log M$ and $0$ where  $-\log M$ stands for complete information loss (i.e., only one model is selected) while $0$ corresponds to no information loss (i.e., uniform belief distribution). Constraining the information loss has succeeded in prior work, for instance in policy search methods such as~\cite{peters2010relative}.
\begin{probdef}[Constrained version]
\begin{equation}
\begin{array}{rrclcl}
\displaystyle Q_i(s,a) = \max_{b(s,a) \in \mathcal{P}^M} & \multicolumn{3}{l}{R(s,a) + \gamma \sum_m{b}_m(s,a)V_m'(s,a)\nonumber } \\
\text{s.t.} & D_{\mathrm{KL}}\big(b(s,a)\big{\|} u \big)& \leq & \iota_{\max} \\
& \sum_{m=1}^{M} b_m(s,a) & = & 1 \\
\multicolumn{4}{l}{ \forall s,a,i \in \mathcal{S} \times \mathcal{A} \times \{1,\dots,M\}}
\end{array} \nonumber
\end{equation}\label{PROB:constrversion}
\end{probdef}
\noindent By letting $\beta$ be the Lagrangian multiplier associated with the KL constraint, we obtain the Lagrangian
\begin{eqnarray}
L_i &\! = \!& f(s,a;b(s,a)) + \beta (D_{\mathrm{KL}}(b(s,a){\|}u) - \iota_{\max}) \nonumber \\
& &   + \lambda(\sum_m b_m(s,a) - 1).\label{E:lagrangian2}
\end{eqnarray}
Substituting $\beta$ with $\nicefrac{-1}{\eta}$ we note that Equation~\ref{E:lagrangian2} becomes identical to~\ref{E:lagrangian} except for a constant factor. Since we can not solve $\eta$ (or $\beta$) analytically, we obtain an approximate solution by iteratively optimizing $\eta$ (or $\beta$) and $b_m$ subsequently.
\gls{obe} takes its name from the fact that when $\eta > 0$, the \textit{logsumexp} acts as a softmax operator. Such operator is also well known as an \textit{entropic mapping}, as it can be derived from a maximum-entropy principle.

The use of the entropic mapping is not new in \gls{rl}: \cite{pmlr-v70-asadi17a} propose an interesting use of the entropic mapping as a soft-max over the action in the \gls{be}; \cite{peters2010relative} instead obtain it from an entropic regularization over the state-action distribution.

\subsection{Relation to Intrinsic Motivation}
In order to highlight the connection between \gls{obe} and \gls{im}, we reformulate \gls{obe} utilizing the unbiased average of the estimates instead of the logsumexp, and by introducing the resulting exploratory bonus $U$ which includes the positive bias
\begin{eqnarray}
Q_i(s,a)\! &=&\! \overline{R}(s,a) + U(s,a) +  \gamma  \sum_{m=1}^M \frac{V_m'(s,a)}{M} \label{bonusBE}
\end{eqnarray}
with
\begin{equation}
U(s,a) \! =\!  \frac{1}{\eta}\log\sum_{m=1}^M \frac{e^{\eta\gamma V_m'(s,a)}}{M} - \gamma  \sum_{m=1}^M \frac{V_m'(s,a)}{M}. \label{bonusdef}
\end{equation}
Noticing that $\dfrac{\sum_{i=1}^N e^{\eta x_i}}{N}$ is the \textit{sample moment generator} w.r.t. samples $\{x_i\}_{i=1}^N$ we can rephrase the exploration bonus as
\begin{eqnarray}
U(s,a)& = & \lim_{N \to +\infty}\frac{1}{\eta} \log [  1 + \sum_{n=2}^{N} \frac{(\eta\gamma)^n}{n!}\mathcal{M}_n(s,a)] \nonumber \\
& = & \eta \gamma \mathcal{M}_2(s,a)  + O(\eta^2)\label{bonus}
\end{eqnarray}
where $\mathcal{M}_n$ is the $n$\textsuperscript{th} central moment of the random variable $V_m'$
\begin{equation}
\mathcal{M}_n(s,a) = M^{-1} \sum_{m=1}^M [( V_m'(s,a) -\overline{V}(s,a))^n]\nonumber
\end{equation}
with 
\begin{equation}
\overline{V}(s,a) = M^{-1} \sum_{m=1}^M V_m'(s,a).\nonumber
\end{equation}
Equation \eqref{bonusBE} shows that \gls{obe} is equivalent to \gls{be} with an additional bonus defined by Equation~\ref{bonus}. The bonus $U$ (for any positive $\eta$) is always positive, and provides a measure of the uncertainty w.r.t. $Q$. This is why \gls{obe} can be interpreted as a special principled form of \gls{im}.

\subsubsection{Explicit exploration} A general problem affecting \gls{im} algorithms, is that the policy greedy to the obtained $Q$-value function, is not optimized for the original problem. As a solution to this issue we approximate two functions: $\tilde{Q}$, which will be updated using the true reward and $Q_E$ which will be updated using only the intrinsic reward \cite{szita2008many}. In this way we obtain both the \gls{im} policy $\pi_o(s) = \arg \max_{a} \tilde{Q}(s,a) + Q_E(s,a)$ and the classic policy  $\pi_u(s) = \arg \max_{a} \tilde{Q}(s,a)$.
Define 
\begin{eqnarray}
	\tilde{Q}_i(s,a) = R(s,a) + \gamma \sum_{m=1}^M\frac{\tilde{V}_m'(s,a)}{M} \quad \text{with} \\
	\tilde{V}_m'(s,a) =  \sum_{s'} P(s'|s,a)\max_{a'}\tilde{Q}_m(s',a')
\end{eqnarray}
to obtain an unbiased estimate of the $Q$-value function, yielding
\begin{eqnarray}
Q_E(s,a) & = & \sum_{t=0}^T \gamma^t U(s_t,a_t) \quad \text{where} \quad s_0 = s, a_0 = a \nonumber \\
& = & \eta^{-1}\log \frac{\sum_{k=1}^M e^{\eta\gamma\max_{a'}\tilde{Q}_k(s',a') + Q_E(s',a')}}{M}\nonumber \\
& &  -  \frac{\sum_{k=1}^M \gamma\max_{a'}\tilde{Q}_k(s',a')}{M}.
\end{eqnarray}
By a simple equation rearrangement, it is possible to show that $\tilde{Q}_i(s,a) + Q_E(s,a)$ is equivalent to $Q_i(s,a)$ as defined in the \gls{obe}~\ref{OBE}.
 
\section{Optimistic value function estimators}
The \gls{obe} offers a theoretical framework in which it is possible to develop optimistic value based algorithms. In fact, \gls{obe} enjoys all the desirable properties of the \gls{be} (e.g. max-norm contractivity), as shown in the supplement. We present briefly two practical applications of the OBE that are optimistic variants of $Q$-learning and \gls{dqn} that we call, respectively, \gls{oql} and \gls{odqn}.
\subsection{Optimistic $Q$-Learning.}
Motivated by the idea of employing an ensemble of regressors as is done in \gls{bdqn}~\cite{osband2017deep}, we assume to have $M$ randomly initialized $Q$-tables. Inspired by the well known $Q$-learning update rule, we derive an optimistic version which is consistent with the \gls{obe} as follows:
\begin{align*}
      \small  Q_{i, t+1}(s,a) &= (1-\alpha_t)Q_{i,t}(s,a)  \nonumber  \\
  & \small + \alpha_t (r_t + \frac{1}{\eta} \log M^{-1} \sum_{j=1}^M e^{\gamma \max_{a'} Q{j,t}(s', a')}).\nonumber
\end{align*}
\label{def:optimistic_qlearning}
We show that, with the update rule proposed, given infinite visits of each state-action pair, all the tables will converge to the same values, and more precisely, after each update, the $n$-th central moment of the updated cell is scaled exactly by $(1 - \alpha_t)^n$:
\begin{equation}
	\mathcal{M}_{n,t+1}(s,a) = (1-\alpha_t)^n \mathcal{M}_{n,t}(s,a) \label{momentdecreasing}
\end{equation}
where 
\begin{equation}
	\mathcal{M}_{n,t}(s,a) = M^{-1} \sum_{i=1}^M (Q_{i,t}(s,a) - \sum_{k=1}^M \frac{Q_{k,t}(s,a)}{M})^n. \nonumber
\end{equation}
This implies that a cell updated $N$ times, with learning rates $\{\alpha_i\}$, will have the $n$-th central moments scaled by $\Pi_{\alpha_i}(1-\alpha_i)^n$ w.r.t.\ the initial one. This leads us to some interesting considerations: 1) the bonus decrease accordingly to the number of state visits; 2) differing from several count-based approaches, our algorithm takes into account the impact of the learning rate; 3) in the limit of an infinite number of visits, the exploration bonus converges to zero.
Further details, including a proof of convergence, are given in the supplemental material\footnote{We based our convergence proof for \gls{oql} on the works of~\cite{melo2001convergence} and~\cite{jaakkola1994convergence}.}.
All the considerations done so far provide a deeper insight about how the algorithm works and its properties. However, in a more complex settings, (e.g., function approximation) the convergence to zero of the exploratory bonus is not guaranteed in general.

\subsection{Optimistic Deep $Q$-Network}\label{sec:proposedalg}
In addition to the novel \gls{oql} algorithm described previously that can be used for limited discrete state spaces, we
propose another algorithm for continuous state spaces based on our \gls{obe}. We take inspiration from the framework provided by \gls{bdqn}~\cite{osband2017deep}
that uses an ensemble of neural networks as estimator for the $Q$ value function.
\gls{bdqn} minimizes the loss
\begin{equation}
	\mathcal{L}_B(s,a)\! = \! \sum_{k=1}^M (r + \gamma \max_{a'}Q_k^T(s',a') - Q_k(s,a))^2, \nonumber
\end{equation}
where $Q_k^T$ is the target network of the $k$\textsuperscript{th} approximator. 
To get an unbiased performance evaluation, we decided to update $M-1$ components of the ensemble with the update rule provided by \gls{bdqn}. We make this choice in order to maintain diversity between the approximations of the ensemble as shown in~\cite{osband2016deep}. We use the remaining single component of the ensemble to approximate $Q_E$. Using the first component to approximate $Q_E$, we get for our new algorithm \gls{odqn} the loss
\begin{eqnarray}
	&\mathcal{L}_O(s,a) = (\eta^{-1}\log \frac{\sum_{k=2}^M
	 e^{\eta\gamma\max_{a'}Q_k^T(s',a')+ Q_1^T(s',a')}}{M}\nonumber \\
&\qquad\qquad\;\;\;\;  -  \frac{\sum_{k=2}^M  \gamma\max_{a'}Q^T_k(s',a')}{M} -Q_1(s,a))^2\nonumber \\
&\;  + \sum_{k=2}^M (r + \gamma\max_{a'}Q^T_k(s',a') -
	 Q_k(s,a))^2.  \label{optimisticloss}
\end{eqnarray}
The exploratory bonus represented by $Q_E = Q_1$ in the proposed \gls{oql} and \gls{odqn} algorithms is needed to
guide exploration during learning. During evaluation, we use majority voting on the remaining $M-1$
components $\{Q_k\}_{k=2}^M$. While we always select an optimistic policy in \gls{oql} during the training phase, in
\gls{odqn} the neural network function approximator may have problems learning to approximate the optimal
policy: if there are not enough unbiased samples the approximator may learn to model only the optimistic
biased samples. Note that in the tabular case, this is not a problem since there is no $Q$-function
approximation. In order to mitigate this problem, we introduce a hyper-parameter $\chi$ which denotes the probability to select an optimistic policy $\pi_o$ in place of the unbiased one $\pi_u$. In this way, we can balance the number of unbiased and optimistic samples.
Algorithm \ref{optimisticdqn} shows the pseudocode of \gls{odqn}. 
\begin{algorithm}[t]
	\caption{Optimistic Deep $Q$-Network}
	\label{optimisticdqn}
	\begin{algorithmic}
		\STATE \textbf{Input:} $\{Q_k\}_{k=1}^K$, $\iota_{\max}$, $\eta_{\mathrm{init}}$, $\chi$, $N$, $C$
		\STATE Let $B$ be a replay buffer storing the experience for training.
		\STATE $\eta = \eta_{\mathrm{init}}$.
		\STATE Let $i \sim \mathrm{Uniform}\{1 \dots M\}$ and $\psi = 1$ w.p. $\chi$ otherwise $\psi = 0$
		\FOR{$N$ epochs}
		\FOR{$C$ steps}
		\STATE Observe $s$
		\STATE Choose $a = \arg \max_a Q_i(s,a) + \psi Q_1(s,a)$
		\STATE Observe reward $r$, next state $s'$, end of episode $t$
		\STATE If $t$ is terminal, $i \sim \mathrm{Uniform}\{2 \dots M\}$ and \\ \ \ \ \ \ \ $\psi = 1$ w.p. $\chi$ otherwise $\psi=0$
		\STATE Store $<s,a,r,s',t>$ in buffer $B$
		\STATE Sample mini-batch $B_{\mathrm{batch}}$
		\STATE Update $\{Q_k\}_{k=1}^K$ using equation \eqref{optimisticloss}
		\STATE $V \leftarrow V + |$ violated constraints \eqref{batchconstr} in $B_{\mathrm{batch}}|$ 
		\ENDFOR
		\STATE Let $\rho = \frac{V}{C * \mathrm{batch\_size}}$
		\STATE Update $\eta$ by \eqref{etaupdate}
		\STATE Update target network
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\paragraph{Automatic hyper-parameter adaptation}
\label{subsec:adaptive}
Recalling that the regularization coefficient $\eta$ in the \gls{obe} is hard to tune, we want to focus our attention on Problem~\ref{PROB:constrversion}. We propose a way, inspired by~\cite{schulman2017proximal}, to optimize $\eta$.
One of the optimization techniques proposed in this work is to measure the ``degree'' of constraint violation and to update the Lagrangian multiplier accordingly. We have to adapt the technique to multiple constraints, as the problem is defined for each state-action pair: we count the number of times the constraints have been violated and then update $\eta$. In more detail, suppose to have $N$ state-action pairs and for each pair $(s_i, a_i)$
\begin{eqnarray}
\sum_m b_m(s_i,a_i) (\log b_m(s_i,a_i) + \log M) \leq \iota_{\max}, \label{batchconstr}
\end{eqnarray}
where $\iota_{\max}$ is defined in Problem~\ref{constrversion}, while $b_m(s_i,a_i)$ is defined by \eqref{pm}. We define $\rho$ as the ratio of violated constraints. We update $\eta$ according to the following rule
\begin{equation}
\eta_{T+1} = \frac{\eta_{T}}{(0.5 +  10 \rho)} \label{etaupdate}.
\end{equation}
In \gls{odqn}, we decided to count the number of constraints violated every $C$ time-steps (basically every update of the target network), using the samples of all the extracted mini-batches. See Algorithm~\ref{optimisticdqn} for further details.  
  
\paragraph{Ensuring a prior distribution} As already discussed, it is important to maintain diversity in our ensemble, and this diversity should reflect the degree of uncertainty. For this reason, we should introduce a sort of prior distribution, as happens in the Bayesian framework. In the case of \gls{oql}, we observe that it is sufficient to randomly initialize each element of the ensemble, since diversity between estimates is a sufficient condition to obtain positive bonus.
For \gls{odqn}, as is done in \gls{bdqn}, we choose to maintain the diversity between approximation, by a random initialization of each component's parameters and by using the bootstrapping technique, so by adding a \textsl{mask} in the replay memory which is sampled by  and use different data samples per regressor.

\section{Experimental evaluation}
\label{S:odqn_experiments}
In the experiments, we compare in the tabular $Q$-function case our novel \gls{oql} with the well-known state-of-the-art bootstrapped $Q$-learning method (BQL) \cite{osband2016deep}, classical $Q$-learning~\ref{S:FQI}, and $Q$-learning with optimistic initialization~\cite{sutton1998reinforcement} in the $50$-Chain \cite{osband2016deep}, Taxi (also known as Maze) \cite{dearden1998bayesian} and Frozen Lake \cite{brockman2016openai} environments. In the case of neural networks approximation, we compare our \gls{odqn} method with \gls{bdqn} and classical \gls{dqn} in the Taxi and Acrobot environments already described in the previous chapters.
The environments are chosen to cover different types of dynamics, have sparse rewards, and include both discrete and continuous states. For Acrobot and Frozen Lake, we used the implementation provided by OpenAI Gym~\cite{brockman2016openai}. First, we will discuss the environments in more detail, then we will provide some details on the
initialization of the methods, and finally finish with an analysis of the results.

The \textbf{N-Chain} environment \cite{osband2016deep} requires a long sequence of non-rewarding actions to achieve the optimal reward. The \gls{mdp} consists of a chain with $N$ states $\{s_i\}_{i=1}^N$, and two actions that move the agent to state $s_{i+1}$ or $s_{i-1}$. The agent always starts in state $s_2$. In state $s_1$ the agent observes a small reward $r(s_1)=1/1000$, while in the $N$\textsuperscript{th} state the agent observes $r(s_N) = 1$. The reward function is zero elsewhere. The agent needs to explore until reaching state $s_N$ even if state $s_1$ looks promising.
   
The \textbf{Frozen Lake} environment is a $8\times 8$ grid-world, in which the agent has to reach a goal position without falling into some holes. The stochastic perturbation of the agent's movement makes the environment challenging.

This problem consists of modeling a car which is placed in a valley, and the power of its engine is not enough to reach the top of the mountain \cite{moore1990efficient}. The car needs to build momentum by driving backward. We use the implementation provided by OpenAI Gym \cite{brockman2016openai}.

\begin{figure*}[t]
  \centering
  \begin{tikzpicture}[scale=1, transform shape]
  \node[state]             (1) {1};
  \node[state, right=of 1] (2) {2};
  \node[state, right=of 2] (3) {\dots};
  \node[state, right=of 3] (4) {N};
  \draw[every loop]
  (1) edge[loop above] node {b, $0.001$} (1)
  (1) edge[bend left, auto=left] node {a} (2)
  %(2) edge[loop above] node {a} (2)
  (2) edge[bend left, auto=left] node {a} (3)
  %(3) edge[loop above] node {a} (3)
  (3) edge[bend left, auto=left] node {a} (4)
  %(1) edge[loop below] node {b} (1)
  (2) edge[bend left, auto=left] node {b} (1)
  %(2) edge[loop below] node {b} (2)
  (3) edge[bend left, auto=left] node {b} (2)
  %(3) edge[loop below] node {b, $1$} (3);
  (4) edge[bend left, auto=left] node {b} (3)
  (4) edge[loop above] node {a, $1$} (4);
  \end{tikzpicture}\caption{Illustration of the environments, from the left to the right: $N$-Chain,  Taxi, Frozen Lake and Acrobot. }
\end{figure*}

\paragraph{Initialization of the $Q$-functions} In the tabular setting, for optimistically initialized Q-learning (OIQL), we initialize the $Q$-function to $15$ in Taxi and to $1$ in $N$-Chain and Frozen Lake. For the other algorithms, we initialize $Q(s,a) \sim \mathcal{N}(\mu=0,\sigma=2)$, except $Q_E$ of \gls{oql} is initialized to $0$.
In the taxi environment, for both \gls{odqn} and \gls{bdqn}, we use a shared convolutional layer with multiple heads as described in \cite{osband2016deep}. For the Acrobot, each component of the ensemble corresponds to a one-layer neural network. For \gls{odqn}, in both the environment, we initialize the output layer corresponding to $Q_E$ to small values of the parameters, in order to obtain initially $Q_E \approx 0$. 

\paragraph{Hyper-parameters tuning} For the tabular settings, we did not run any hyper-parameter optimization. With neural networks we performed a small grid search over the number of neurons of the network, and whether to use bootstrapping or not. More specifically, we selected hyper-parameters maximizing the mean return averaged over the whole learning curve, using $20$ different seeds. In the plots; we compare \gls{odqn} and \gls{bdqn} using the best hyper-parameter setup found for \gls{bdqn}. In \gls{oql} we use $\eta=10$ and for \gls{odqn}, we use $\chi=0.25$ and $\iota_{\max}=1$. For further details about the implementation of the algorithms, and the grid search, please see the Supplement.

\setlength\figureheight{4cm}
\setlength\figurewidth{4cm}
% \begin{figure*}[t]
% 	\input{img/performances.tex}
% 	\caption{The first row shows the average return for each tabular algorithm in the $N$-Chain, Taxi, and
% 	 Frozen Lake environments and for each neural network based algorithm in the Taxi and Acrobot environments
% 	 together with $95\%$ confidence intervals. The
%      second row shows the distribution over the number of time steps before observing the maximum reward in the \gls{mdp}.}
% 	\label{results1}
% \end{figure*}
\subsection{Results}
Figure~\ref{results1} summarizes the results obtained by averaging over $64$ different seed the tabular algorithms, and $100$ seeds \gls{dqn} and BDEN (more details, e.g. results for different hyper-parameter 
settings, can be found in the supplement).
\gls{oql} learns faster than the other tabular algorithms. In the Chain environment, and in Taxi,
our algorithm \gls{oql} finds the highest reward faster than \gls{bql} or \gls{ql}. On the other hand, also OIQL seems to find
high rewards fast, as shown in the box-plots. However, OIQL requires more training epochs to escape the high initial
optimistic values of the value function. In contrast, \gls{oql} finds high values fast in all the problems
($50$-Chain, Taxi, Frozen Lake) using the optimistic Bellman equation
while converging to a near-optimal solution as suggested by our convergence proofs.

With neural network approximation of the value function, \gls{odqn} outperforms \gls{bdqn} in the Taxi environment
demonstrating that the principles of \gls{obe} work even with function approximation. In the Cartpole environment,
the learning curves of \gls{odqn} and \gls{odqn} are nearly identical, possibly, due to the simplicity of the environment.
