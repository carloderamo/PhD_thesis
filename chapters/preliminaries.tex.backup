\chapter{Preliminaries}
\gls{rl} is intuitively explainable as the process of learning from interaction with the environment, but this hasty explanation is only a very high level description of it; indeed, a more formal way to model the problem is required to properly analyze it. This chapter provides a description of the mathematical framework required to model \gls{rl}. It also explains a selection of algorithms that are related to the work done in this thesis in order to provide enough knowledge about the literature I dealt with during my years of Ph.D. research.

\section{Agent and environment}
\begin{figure}[b]
\begin{minipage}{\textwidth}
\begin{center}
  \includegraphics[scale=.75]{img/mdp1.jpg}
\end{center}
\end{minipage}
\caption[Reinforcement Learning problem scheme]{The scheme of a \gls{rl} model.}\label{F:mdp1}
\end{figure}
The interaction of an agent inside an environment can be seen as the execution of actions to move itself in the environment and observing the consequences of its actions (Figure \ref{F:mdp1}). The temporal progress of the interaction is modeled in a set of discrete time steps $t_i$ where the agent sees a representation $s_i$ of the environment, executes an action $a_i$ and observes the new representation of the environment $s_{i+1}$. The problems about observation and interaction discussed in Chapter~\ref{C:intro} are simplified by an explicit selection of data to observe from the environment and of the executable actions. In this way, only the relevant aspect of the sensory data acquired from the environment the agent are used. Together with $s_{i+1}$, the agent also sees a return $r_i$ which is not given by the environment, but it is a measure considered by the agent to evaluate the convenience of the consequences of the actions it executes. The total number of time steps is called \textit{horizon} $H$ and determines a first taxonomy of problems:
\begin{itemize}
 \item finite time horizon: $t_i, \forall i \in [0, 1, 2, \dots, H)$;
 \item infinite time horizon: $t_i, \forall i \in [0, 1, 2, \dots, \infty)$.
\end{itemize}
Some problems can terminate before reaching the horizon, which happens when the agent reaches special situations called \textit{absorbing} states. These states are usually desirable or catastrophic states when the interaction of the agent with the environment is no more useful or impossible. The set of steps between the start of the interaction to the end is called \textit{episode}.

The interaction of the agent with the environment is performed with the purpose to reach a goal for which the agent has been designed. The way to give the knowledge of the goal to the agent is to provide it with a measure of the quality of its behavior. This measure is called \textit{reward} and is a function usually returning a real scalar value given the observation of the current state of the agent. The goal of the agent is to maximize a measure related to the collected rewards. In an infinite time horizon problem it can be:
\begin{itemize}
 \item cumulative reward:
 \begin{equation}\label{E:sumrew}
  J = \sum_{t=0}^\infty r_t;
 \end{equation}
\item average reward:
\begin{equation}
 J = \lim_{n\to\infty}\dfrac{\sum_{t=0}^n r_t}{n};
\end{equation}
\item discounted cumulative reward:
\begin{equation}\label{E:discumrew}
 J = \sum_{t=0}^\infty \gamma^t r_t.
\end{equation}
\end{itemize}
The measure in Equation~\ref{E:discumrew} uses a real scalar $\gamma \in (0, 1]$ which has the purpose to give different importance to rewards w.r.t. the time step they have been collected. If $\gamma = 1$ the equation reduces to~\ref{E:sumrew}, whereas the smaller it becomes the less the agent cares about rewards far in time.

\section{Markov Decision Processes}
\begin{figure}[t]
\begin{minipage}{\textwidth}
\begin{center}
  \includegraphics[scale=.6]{img/mdp2.png}
\end{center}
\end{minipage}
\caption[Reinforcement Learning problem scheme]{The scheme of a \gls{rl} model.}\label{F:mdp1}
\end{figure}
The mathematical framework to study the interaction of the agent with the environment is provided by the theory behind \glspl{mdp}.
